{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_KoGPT2_Summarizer(Chatbot).ipynb",
      "provenance": [],
      "mount_file_id": "1gWO2WCMgHXjeA0eUpe_AfU3dqN54kWd_",
      "authorship_tag": "ABX9TyPG0W4mgSUYAarUiAAKFBoI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dev-hottae/PINC/blob/master/Data_Analysis/Train_KoGPT2_Summarizer(Chatbot).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o06u2PH25og",
        "outputId": "8c8c9c1d-a5a9-46fd-811d-a7ef610dd6ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls drive/'My Drive'/'Colab Notebooks'/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " __MACOSX\t  'report.ipynb의 사본의 사본'\t     requirements.txt\n",
            " NarrativeKoGPT2  'report.ipynb의 사본의 사본 (1)'   Untitled0.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGhAE5Ac3E6G",
        "outputId": "7dc64feb-fd3d-4739-a9d1-7b3963f78d24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -r drive/'My Drive'/'Colab Notebooks'/requirements.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gluonnlp>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/81/a238e47ccba0d7a61dcef4e0b4a7fd4473cb86bed3d84dd4fe28d45a0905/gluonnlp-0.10.0.tar.gz (344kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 2.6MB/s \n",
            "\u001b[?25hCollecting mxnet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/bb/54cbabe428351c06d10903c658878d29ee7026efbe45133fd133598d6eb6/mxnet-1.7.0.post1-py2.py3-none-manylinux2014_x86_64.whl (55.0MB)\n",
            "\u001b[K     |████████████████████████████████| 55.0MB 55kB/s \n",
            "\u001b[?25hCollecting sentencepiece>=0.1.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 48.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/Colab Notebooks/requirements.txt (line 4)) (1.6.0+cu101)\n",
            "Collecting transformers>=2.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/4e/4f1ede0fd7a36278844a277f8d53c21f88f37f3754abf76a5d6224f76d4a/transformers-3.4.0-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 46.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/Colab Notebooks/requirements.txt (line 6)) (4.41.1)\n",
            "Collecting kss\n",
            "  Downloading https://files.pythonhosted.org/packages/fc/bb/4772901b3b934ac204f32a0bd6fc0567871d8378f9bbc7dd5fd5e16c6ee7/kss-1.3.1.tar.gz\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/requirements.txt (line 1)) (1.18.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/requirements.txt (line 1)) (0.29.21)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/requirements.txt (line 1)) (20.4)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet->-r drive/My Drive/Colab Notebooks/requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->-r drive/My Drive/Colab Notebooks/requirements.txt (line 4)) (0.16.0)\n",
            "Collecting tokenizers==0.9.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a5/78be1a55b2ac8d6a956f0a211d372726e2b1dd2666bb537fea9b03abd62c/tokenizers-0.9.2-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 21.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/requirements.txt (line 5)) (3.12.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/requirements.txt (line 5)) (0.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/requirements.txt (line 5)) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 47.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/requirements.txt (line 5)) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/requirements.txt (line 1)) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/Colab Notebooks/requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/Colab Notebooks/requirements.txt (line 2)) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/Colab Notebooks/requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/Colab Notebooks/requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/requirements.txt (line 5)) (50.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/requirements.txt (line 5)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/requirements.txt (line 5)) (0.17.0)\n",
            "Building wheels for collected packages: gluonnlp, kss, sacremoses\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp36-cp36m-linux_x86_64.whl size=588539 sha256=af43c4ff58f5406511031348cc1d782bd59ba07271271f2fbe75d1dbe424423d\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/65/52/63032864a0f31a08b9a88569f803b5bafac8abd207fd7f7534\n",
            "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kss: filename=kss-1.3.1-cp36-cp36m-linux_x86_64.whl size=251564 sha256=f63f57ecb39400946345d9d0cc0f4c9aa3a7bcd51b17053385f011b46aecacc2\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/98/d1/53f75f89925cd95779824778725ee3fa36e7aa55ed26ad54a8\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=bc0f5ea4db1a8c12713d9049e7cb3ebef9c72ddba4cbe09924ba01c997b683a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built gluonnlp kss sacremoses\n",
            "Installing collected packages: gluonnlp, graphviz, mxnet, sentencepiece, tokenizers, sacremoses, transformers, kss\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed gluonnlp-0.10.0 graphviz-0.8.4 kss-1.3.1 mxnet-1.7.0.post1 sacremoses-0.0.43 sentencepiece-0.1.94 tokenizers-0.9.2 transformers-3.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXfN9X9Q3GDC",
        "outputId": "c1bdaabe-5233-4757-a63e-860bf26d2d6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install git+https://github.com/SKT-AI/KoGPT2.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/SKT-AI/KoGPT2.git\n",
            "  Cloning https://github.com/SKT-AI/KoGPT2.git to /tmp/pip-req-build-8c07hr6y\n",
            "  Running command git clone -q https://github.com/SKT-AI/KoGPT2.git /tmp/pip-req-build-8c07hr6y\n",
            "Building wheels for collected packages: kogpt2\n",
            "  Building wheel for kogpt2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kogpt2: filename=kogpt2-0.1.1-cp36-none-any.whl size=14054 sha256=2540bcb62fe5a78e04d31e73ebec9f02e1168e75e394a1e7b0883ff9799ef16e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jctobdga/wheels/3b/a2/30/432bb7490a2ea23a90049e6c5725f6acd7e925f1abfb3d7ddf\n",
            "Successfully built kogpt2\n",
            "Installing collected packages: kogpt2\n",
            "Successfully installed kogpt2-0.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "botJm63U3Hfb",
        "outputId": "be28ff74-7314-472f-c766-e27f5b2166a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkQBHaRU3I22"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from gluonnlp.data import SentencepieceTokenizer \n",
        "import gluonnlp as nlp\n",
        "import pandas as pd\n",
        "import mxnet as mx\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from mxnet import gluon, nd\n",
        "from mxnet.gluon import nn"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbfck1c03KMY"
      },
      "source": [
        "from kogpt2.utils import download, tokenizer, get_tokenizer\n",
        "from kogpt2.pytorch_kogpt2 import GPT2Config, GPT2LMHeadModel"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yzj1Bzbr3LOR",
        "outputId": "ff8baba8-f66b-48f7-b74a-e91314facf4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#GPU 사용\n",
        "device = torch.device(\"cuda:0\")\n",
        "device"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FySFrqaB3MQb"
      },
      "source": [
        "ctx= 'cuda'#'cuda' #'cpu' #학습 Device CPU or GPU. colab의 경우 GPU 사용\n",
        "cachedir='~/kogpt2/' # KoGPT-2 모델 다운로드 경로\n",
        "save_path = '/content/drive/My Drive/머신러닝/팀 프로젝트/06. AI를 이용한 금융 보고서/Data_Analysis/checkpoint/'\n",
        "\n",
        "pytorch_kogpt2 = {\n",
        "    'url':\n",
        "    'https://kobert.blob.core.windows.net/models/kogpt2/pytorch/pytorch_kogpt2_676e9bcfa7.params',\n",
        "    'fname': 'pytorch_kogpt2_676e9bcfa7.params',\n",
        "    'chksum': '676e9bcfa7'\n",
        "}\n",
        "kogpt2_config = {\n",
        "    \"initializer_range\": 0.02,\n",
        "    \"layer_norm_epsilon\": 1e-05,\n",
        "    \"n_ctx\": 1024,\n",
        "    \"n_embd\": 768,\n",
        "    \"n_head\": 12,\n",
        "    \"n_layer\": 12,\n",
        "    \"n_positions\": 1024,\n",
        "    \"vocab_size\": 50000,\n",
        "    \"output_past\": None\n",
        "}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBis601M3PzN",
        "outputId": "c480dba5-8123-47e0-d30b-da6e423014db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# download model\n",
        "model_info = pytorch_kogpt2\n",
        "model_path = download(model_info['url'],\n",
        "                       model_info['fname'],\n",
        "                       model_info['chksum'],\n",
        "                       cachedir=cachedir)\n",
        "# download vocab\n",
        "vocab_info = tokenizer\n",
        "vocab_path = download(vocab_info['url'],\n",
        "                       vocab_info['fname'],\n",
        "                       vocab_info['chksum'],\n",
        "                       cachedir=cachedir)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[██████████████████████████████████████████████████]\n",
            "[██████████████████████████████████████████████████]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwBph5563RKq"
      },
      "source": [
        "# KoGPT-2 언어 모델 학습을 위한 GPT2LMHeadModel 선언\n",
        "kogpt2model = GPT2LMHeadModel.from_pretrained(pretrained_model_name_or_path=None,\n",
        "                                              config=GPT2Config.from_dict(kogpt2_config),\n",
        "                                              state_dict=torch.load(model_path))\n",
        "\n",
        "device = torch.device(ctx)\n",
        "kogpt2model.to(device)\n",
        "# Fine Tunning을 위해 train 선언\n",
        "kogpt2model.train()\n",
        "# 단어 뭉치 가져오기\n",
        "vocab_b_obj = nlp.vocab.BERTVocab.from_sentencepiece(vocab_path,\n",
        "                                                     mask_token=None,\n",
        "                                                     sep_token=None,\n",
        "                                                     cls_token=None,\n",
        "                                                     unknown_token='<unk>',\n",
        "                                                     padding_token='<pad>',\n",
        "                                                     bos_token='<s>',\n",
        "                                                     eos_token='</s>')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tVhP3aq3SR9",
        "outputId": "d2c5d0af-9c0a-461a-c870-d4ada17258e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tok_path = get_tokenizer()\n",
        "vocab = vocab_b_obj\n",
        "sentencepieceTokenizer = SentencepieceTokenizer(tok_path, 0, 0)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using cached model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk58PGPX3TfW",
        "outputId": "107d846a-d805-44da-aeae-a571df8b0d91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "source": [
        "data_path = '/content/drive/My Drive/머신러닝/팀 프로젝트/06. AI를 이용한 금융 보고서/Data_Analysis/DataSet/Sentence_Word.csv'\n",
        "news_data = pd.read_csv(data_path, encoding='utf-8')\n",
        "news_data.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>역시 큰 사고 때면 어김없이 동포애를 재확인할 수 있군요.</td>\n",
              "      <td>사고 때 동포 애 확인 수</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>이번 지진ㆍ해일 참사에 희생된 한국인들과 수색작업에 참가한 사람들을 도우려는 온정의...</td>\n",
              "      <td>이번 지진 참사 희생 한국인 수색 작업 참가 사람 온정 손길 것 현장 지휘 본부 관...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>푸껫 시내 로열 푸껫 시티 호텔 2층에 마련된 현장지휘본부에는 새해 첫날에도 불구하...</td>\n",
              "      <td>푸껫 시내 로열 푸껫 시티 호텔 층 마련 현장 지휘 본부 새해 첫날 현지 교민 자원...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>또 이번 사태로 비상근무체제에 돌입한 방콕의 한국대사관과 한인회 등에도 위문품 전달...</td>\n",
              "      <td>이번 사태 비상근무 체제 돌입 방콕 한국 대사관 인회 등 위문품 전달 방법 문의 전화</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>푸껫에 거주하는 800여 명의 교민들은 이번 사태로 주업인 관광업이 한동안 상당한 ...</td>\n",
              "      <td>푸껫 거주 명 교민 이번 사태 주업 관광업 한동안 상당 타격 것 우려 채 한국인 희...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Sentence                                               Word\n",
              "0                   역시 큰 사고 때면 어김없이 동포애를 재확인할 수 있군요.                                     사고 때 동포 애 확인 수\n",
              "1  이번 지진ㆍ해일 참사에 희생된 한국인들과 수색작업에 참가한 사람들을 도우려는 온정의...  이번 지진 참사 희생 한국인 수색 작업 참가 사람 온정 손길 것 현장 지휘 본부 관...\n",
              "2  푸껫 시내 로열 푸껫 시티 호텔 2층에 마련된 현장지휘본부에는 새해 첫날에도 불구하...  푸껫 시내 로열 푸껫 시티 호텔 층 마련 현장 지휘 본부 새해 첫날 현지 교민 자원...\n",
              "3  또 이번 사태로 비상근무체제에 돌입한 방콕의 한국대사관과 한인회 등에도 위문품 전달...    이번 사태 비상근무 체제 돌입 방콕 한국 대사관 인회 등 위문품 전달 방법 문의 전화\n",
              "4  푸껫에 거주하는 800여 명의 교민들은 이번 사태로 주업인 관광업이 한동안 상당한 ...  푸껫 거주 명 교민 이번 사태 주업 관광업 한동안 상당 타격 것 우려 채 한국인 희..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWXpMhS23Uzl",
        "outputId": "cba3acbd-a63d-42bd-a473-e34783e4b40f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dataset_train = []\n",
        "for i in tqdm(range(len(news_data))):\n",
        "    dataset_train.append([news_data['Word'][i], news_data['Sentence'][i]])\n",
        "\n",
        "dataset_train[:5]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 722/722 [00:00<00:00, 65263.41it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['사고 때 동포 애 확인 수', '역시 큰 사고 때면 어김없이 동포애를 재확인할 수 있군요.'],\n",
              " ['이번 지진 참사 희생 한국인 수색 작업 참가 사람 온정 손길 것 현장 지휘 본부 관계자 말',\n",
              "  '이번 지진ㆍ해일 참사에 희생된 한국인들과 수색작업에 참가한 사람들을 도우려는 온정의 손길이 끊이지 않은 것을 지켜본 현장지휘본부 관계자의 말이다.'],\n",
              " ['푸껫 시내 로열 푸껫 시티 호텔 층 마련 현장 지휘 본부 새해 첫날 현지 교민 자원 봉사자 진출 기업체 관계자 등 방문',\n",
              "  '푸껫 시내 로열 푸껫 시티 호텔 2층에 마련된 현장지휘본부에는 새해 첫날에도 불구하고 현지 교민 자원봉사자, 진출 기업체 관계자 등의 방문이 잇따랐다.'],\n",
              " ['이번 사태 비상근무 체제 돌입 방콕 한국 대사관 인회 등 위문품 전달 방법 문의 전화',\n",
              "  '또 이번 사태로 비상근무체제에 돌입한 방콕의 한국대사관과 한인회 등에도 위문품 전달 방법을 문의하는 전화가 빗발쳤다.'],\n",
              " ['푸껫 거주 명 교민 이번 사태 주업 관광업 한동안 상당 타격 것 우려 채 한국인 희생자 수색 작업 원활 진행 통역 안내 등 자원 봉사',\n",
              "  '푸껫에 거주하는 800여 명의 교민들은 이번 사태로 주업인 관광업이 한동안 상당한 타격이 불가피할 것이라는 우려를 잠시 뒤로한 채 한국인 희생자 수색작업을 원활하게 진행하기 위해 통역, 안내 등의 자원봉사를 아끼지 않고 있다.']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maRo8rRf3ZMV"
      },
      "source": [
        "class GPT2Dataset(Dataset):\n",
        "    def __init__(self, data_file, vocab, tokenizer, max_len=64):\n",
        "        self.data = data_file\n",
        "        self.vocab = vocab\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.padder = nlp.data.PadSequence(self.max_len, pad_val=vocab[vocab.padding_token])\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        input = self.data[index][0]\n",
        "        answer = self.data[index][1]\n",
        "        i_toked = ['<usr>', ] + self.tokenizer(input) + ['</s>', ]\n",
        "        i_len = len(i_toked)\n",
        "        a_toked = ['<sys>', ] + self.tokenizer(answer) + ['</s>', ]\n",
        "        a_len = len(a_toked)\n",
        "        if i_len + a_len > self.max_len:\n",
        "            a_len = self.max_len - i_len\n",
        "            if a_len <= 0:\n",
        "                i_toked = i_toked[-(int(self.max_len/2)):]\n",
        "                i_toked = len(i_toked)\n",
        "                a_len = self.max_len - i_len\n",
        "                assert a_len > 0\n",
        "            a_toked = a_toked[:a_len]\n",
        "            a_len = len(a_toked)\n",
        "            assert a_len == len(a_toked), f'{a_len} ==? {len(a_toked)}'\n",
        "        mask = [0] * i_len + [1] * a_len + [0] * (self.max_len - i_len - a_len)\n",
        "        \n",
        "        return (self.padder(vocab[i_toked + a_toked]), nd.array(mask))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lca81tQv3cUv"
      },
      "source": [
        "batch_size = 2\n",
        "news_dataset = GPT2Dataset(dataset_train, vocab, sentencepieceTokenizer)\n",
        "news_data_loader = mx.gluon.data.DataLoader(news_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=3)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMK5ZD8A3dos",
        "outputId": "2c8e2230-116d-411c-8592-e7f6842a3150",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i in news_data_loader:\n",
        "    print(i)\n",
        "    break"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\n",
            "[[    2  3714 10753  1953  1889  3133  1440   509   666  5592  2861   710\n",
            "   7073 13607   133  2127   470  1906  1440   197     1     4 13509 18351\n",
            "  47653 47476  1813  1953  5475  2047  1889  7096 22139 18148   401 47624\n",
            "   9837 47623 47769 47624  2100   710 47623 47671  7073 41058 13494   299\n",
            "   9504   470 47460  5447  1558  7953  1012   123 47452   445 12976   123\n",
            "  47440     1     3     3]\n",
            " [    2   787  2482  4241  8792  1110  1665  2611   447  4241   509   389\n",
            "    903  2143   683  1267     1     4   787 24209   211  4241   108 15138\n",
            "   1110 47443  1665 37580   494  2966  9349  5904  4241  4230  9607   389\n",
            "    903   808 47453  2143 47463   683  6102 27127   202 47440     1     3\n",
            "      3     3     3     3     3     3     3     3     3     3     3     3\n",
            "      3     3     3     3]]\n",
            "<NDArray 2x64 @cpu_pinned(0)>, \n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n",
            "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "<NDArray 2x64 @cpu_pinned(0)>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11_Qd_US3exG"
      },
      "source": [
        "class KoGPT2Chat(nn.HybridBlock):\n",
        "    def __init__(self, kogpt2, prefix=None, params=None):\n",
        "        super(KoGPT2Chat, self).__init__(prefix=prefix, params=params)\n",
        "        self.kogpt2 = kogpt2\n",
        "\n",
        "    def hybrid_forward(self, F, inputs):\n",
        "        # (batch, seq_len, hiddens)\n",
        "        output, _ = self.kogpt2(inputs)\n",
        "        return output"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpcslIlA8jr6"
      },
      "source": [
        "kogptqa = KoGPT2Chat(kogpt2model)\n",
        "kogptqa.hybridize()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-0XVQr074Em"
      },
      "source": [
        "learning_rate = 1e-5\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(kogpt2model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYL1eexN8jPD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7e0HVqZ7vdB",
        "outputId": "cd11b345-83ac-489d-9e23-e237c498aa1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        }
      },
      "source": [
        "epoch = 10\n",
        "for epoch in range(epoch):\n",
        "    count = 0\n",
        "    avg_loss = (0.0, 0.0)\n",
        "    for batch_id, (token_ids, mask) in tqdm(news_data_loader):\n",
        "        if step_num < num_warmupkogptqa = KoGPT2Chat(model)\n",
        "    kogptqa.hybridize()_steps:\n",
        "            new_lr = lr * step_num / num_warmup_steps\n",
        "        else:\n",
        "            non_warmup_steps = step_num - num_warmup_steps\n",
        "            offset = non_warmup_steps / (num_train_steps -\n",
        "                                            num_warmup_steps)\n",
        "            new_lr = lr - offset * lr\n",
        "    #     trainer.set_learning_rate(new_lr)\n",
        "    #     with mx.autograd.record():\n",
        "    #         # load data to GPU or GPU\n",
        "    #         token_ids = token_ids.as_in_context(ctx)\n",
        "    #         mask = mask.as_in_context(ctx)\n",
        "    #         label = label.as_in_context(ctx)\n",
        "    #         # forward computation\n",
        "    #         out = kogptqa(token_ids)\n",
        "    #         masked_out = nd.where(\n",
        "    #             mask.expand_dims(axis=2).repeat(repeats=out.shape[2],\n",
        "    #                                             axis=2), out,\n",
        "    #             neg * nd.ones_like(out))\n",
        "    #         # loss for responses exincluding MASK and PAD\n",
        "    #         ls = loss_function(masked_out, label).sum() / mask.sum()        \n",
        "        \n",
        "    #     optimizer.zero_grad()\n",
        "    #     # Data에 Torch 스택\n",
        "    #     data = torch.stack(data)\n",
        "    #     data = data.transpose(1,0)\n",
        "    #     # 데이터와 모델에 GPU 설정\n",
        "    #     data = data.to(device)\n",
        "    #     kogpt2model = kogpt2model.to(device)\n",
        "    #     # 결과값\n",
        "    #     outputs = kogpt2model(data, labels=data)\n",
        "    #     loss, logits = outputs[:2]\n",
        "    #     loss = loss.to(device)\n",
        "    #     loss.backward()\n",
        "    #     avg_loss = (avg_loss[0] * 0.99 + loss, avg_loss[1] * 0.99 + 1.0)\n",
        "    #     optimizer.step()\n",
        "    #     count+=1\n",
        "\n",
        "    #     if count % 10 == 0:\n",
        "    #         print('epoch no.{0} train no.{1}  loss = {2:.5f} avg_loss = {3:.5f}' . format(epoch+1, count, loss, avg_loss[0] / avg_loss[1]))\n",
        "\n",
        "    # # 10에폭 단위 체크포인트 저장\n",
        "    # if epoch % 10 == 0:\n",
        "    #     torch.save({\n",
        "    #         'model_state_dict': kogpt2model.state_dict(),\n",
        "    #         'optimizer_state_dict' : optimizer.state_dict(),\n",
        "    #         'loss' : loss\n",
        "    #         }, save_path+'summarize_KoGPT2_checkpoint{}.tar'.format(epoch//10))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/361 [00:00<?, ?it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-15b580a4a859>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mstep_num\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnum_warmup_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mnew_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstep_num\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_warmup_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'step_num' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooMgVNlD3idn",
        "outputId": "4a2f8223-8cdb-4310-fa9f-7b4597e28d17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        }
      },
      "source": [
        "for epoch_id in range(10):\n",
        "    step_loss = 0\n",
        "    for batch_id, (token_ids, mask) in enumerate(news_data_loader):\n",
        "        if step_num < num_warmup_steps:\n",
        "            new_lr = lr * step_num / num_warmup_steps\n",
        "        else:\n",
        "            non_warmup_steps = step_num - num_warmup_steps\n",
        "            offset = non_warmup_steps / (num_train_steps -\n",
        "                                            num_warmup_steps)\n",
        "            new_lr = lr - offset * lr\n",
        "        trainer.set_learning_rate(new_lr)\n",
        "        with mx.autograd.record():\n",
        "            # load data to GPU or GPU\n",
        "            token_ids = token_ids.as_in_context(ctx)\n",
        "            mask = mask.as_in_context(ctx)\n",
        "            label = label.as_in_context(ctx)\n",
        "            # forward computation\n",
        "            out = kogptqa(token_ids)\n",
        "            masked_out = nd.where(\n",
        "                mask.expand_dims(axis=2).repeat(repeats=out.shape[2],\n",
        "                                                axis=2), out,\n",
        "                neg * nd.ones_like(out))\n",
        "            # loss for responses exincluding MASK and PAD\n",
        "            ls = loss_function(masked_out, label).sum() / mask.sum()\n",
        "        # backward computation\n",
        "        ls.backward()\n",
        "        if not accumulate or (batch_id + 1) % accumulate == 0:\n",
        "            trainer.allreduce_grads()\n",
        "            nlp.utils.clip_grad_global_norm(params, 1)\n",
        "            trainer.update(accumulate if accumulate else 1)\n",
        "            step_num += 1\n",
        "            if accumulate and accumulate > 1:\n",
        "                # set grad to zero for gradient accumulation\n",
        "                all_model_params.zero_grad()\n",
        "        step_loss += ls.asscalar()\n",
        "        if step_num % log_interval == 0 and step_num > 0:\n",
        "            print(\n",
        "                '[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.10f}, train ppl={:.3f}'\n",
        "                .format(epoch_id + 1, batch_id + 1, len(train_dataloader),\n",
        "                        step_loss / log_interval, trainer.learning_rate,\n",
        "                        math.exp(step_loss / log_interval)))\n",
        "            step_loss = 0\n",
        "    torch.save({\n",
        "            'model_state_dict': kogpt2model.state_dict(),\n",
        "            'optimizer_state_dict' : optimizer.state_dict(),\n",
        "            'loss' : loss\n",
        "            }, save_path+'summarize_KoGPT2_checkpoint{}.tar'.format(epoch//10))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-bba01239e0d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mstep_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mstep_num\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnum_warmup_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m             \u001b[0mnew_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstep_num\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_warmup_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'step_num' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rky8QccJ7VtV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}