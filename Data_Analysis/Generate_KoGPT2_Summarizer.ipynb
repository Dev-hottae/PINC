{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generate_KoGPT2_Summarizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPGQSpgxzQcfAWm1WAngIr1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dev-hottae/PINC/blob/master/Data_Analysis/Generate_KoGPT2_Summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2RUG5By8Fbh"
      },
      "source": [
        "# **요약 생성**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eern4PAG69Ey",
        "outputId": "e1d3ec5c-938c-4ae9-f330-69e801ae6e69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NXcFrrB8Poq"
      },
      "source": [
        "## **패키지 설치**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPn8CLpo5rjS",
        "outputId": "d7acc01d-0532-4f6c-8891-ab7f7ff23b68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!ls drive/'My Drive'/'Colab Notebooks'/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " __MACOSX\t  'report.ipynb의 사본의 사본'\t     requirements.txt\n",
            " NarrativeKoGPT2  'report.ipynb의 사본의 사본 (1)'   Untitled0.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug9ZM3Dz5rRZ",
        "outputId": "2a92eac2-4283-409a-9c9c-5725e56352f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install -r drive/'My Drive'/'Colab Notebooks'/requirements.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gluonnlp>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/81/a238e47ccba0d7a61dcef4e0b4a7fd4473cb86bed3d84dd4fe28d45a0905/gluonnlp-0.10.0.tar.gz (344kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 2.7MB/s \n",
            "\u001b[?25hCollecting mxnet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/bb/54cbabe428351c06d10903c658878d29ee7026efbe45133fd133598d6eb6/mxnet-1.7.0.post1-py2.py3-none-manylinux2014_x86_64.whl (55.0MB)\n",
            "\u001b[K     |████████████████████████████████| 55.0MB 55kB/s \n",
            "\u001b[?25hCollecting sentencepiece>=0.1.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 46.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/Colab Notebooks/requirements.txt (line 4)) (1.6.0+cu101)\n",
            "Collecting transformers>=2.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/4e/4f1ede0fd7a36278844a277f8d53c21f88f37f3754abf76a5d6224f76d4a/transformers-3.4.0-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 49.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/Colab Notebooks/requirements.txt (line 6)) (4.41.1)\n",
            "Collecting kss\n",
            "  Downloading https://files.pythonhosted.org/packages/fc/bb/4772901b3b934ac204f32a0bd6fc0567871d8378f9bbc7dd5fd5e16c6ee7/kss-1.3.1.tar.gz\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/requirements.txt (line 1)) (1.18.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/requirements.txt (line 1)) (0.29.21)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/requirements.txt (line 1)) (20.4)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet->-r drive/My Drive/Colab Notebooks/requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->-r drive/My Drive/Colab Notebooks/requirements.txt (line 4)) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/requirements.txt (line 5)) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 44.4MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a5/78be1a55b2ac8d6a956f0a211d372726e2b1dd2666bb537fea9b03abd62c/tokenizers-0.9.2-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 50.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/requirements.txt (line 5)) (0.7)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/requirements.txt (line 5)) (3.12.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/requirements.txt (line 5)) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/requirements.txt (line 1)) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/Colab Notebooks/requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/Colab Notebooks/requirements.txt (line 2)) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/Colab Notebooks/requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/Colab Notebooks/requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/requirements.txt (line 5)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/requirements.txt (line 5)) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/requirements.txt (line 5)) (50.3.0)\n",
            "Building wheels for collected packages: gluonnlp, kss, sacremoses\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp36-cp36m-linux_x86_64.whl size=588538 sha256=d8468934c41f6648571a2e6d66d990fbd75cdf5ebc659997d9d9df839d831b45\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/65/52/63032864a0f31a08b9a88569f803b5bafac8abd207fd7f7534\n",
            "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kss: filename=kss-1.3.1-cp36-cp36m-linux_x86_64.whl size=251548 sha256=3a60ddbf0ad004f3d06bd6e12af320ded145ca69fb8740cb0d7cc91889fa0de6\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/98/d1/53f75f89925cd95779824778725ee3fa36e7aa55ed26ad54a8\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=30e62e3be8c65157aec22dd1b99eb23a688694ef644226354228b4608b6a9951\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built gluonnlp kss sacremoses\n",
            "Installing collected packages: gluonnlp, graphviz, mxnet, sentencepiece, sacremoses, tokenizers, transformers, kss\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed gluonnlp-0.10.0 graphviz-0.8.4 kss-1.3.1 mxnet-1.7.0.post1 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.2 transformers-3.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cit-CdqO5wRr",
        "outputId": "c1e45d29-1a4e-4e99-f8ba-6562b77c927c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "source": [
        "!pip install git+https://github.com/SKT-AI/KoGPT2.git"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/SKT-AI/KoGPT2.git\n",
            "  Cloning https://github.com/SKT-AI/KoGPT2.git to /tmp/pip-req-build-owt9vs7y\n",
            "  Running command git clone -q https://github.com/SKT-AI/KoGPT2.git /tmp/pip-req-build-owt9vs7y\n",
            "Building wheels for collected packages: kogpt2\n",
            "  Building wheel for kogpt2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kogpt2: filename=kogpt2-0.1.1-cp36-none-any.whl size=14054 sha256=ce9b638fc57254e9b6be042b86cecb9cf0eb4b7156b1f0cdb1fb720a65d3885e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fvcz_dg7/wheels/3b/a2/30/432bb7490a2ea23a90049e6c5725f6acd7e925f1abfb3d7ddf\n",
            "Successfully built kogpt2\n",
            "Installing collected packages: kogpt2\n",
            "Successfully installed kogpt2-0.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cocq5eIL5x7K",
        "outputId": "24ad2d2e-8610-483c-f166-cae4bdfc7424",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he8VoLDF8RkB"
      },
      "source": [
        "## **Import 패키지**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3UWB4Go5zM7"
      },
      "source": [
        "import torch, random\n",
        "import gluonnlp as nlp\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from gluonnlp.data import SentencepieceTokenizer \n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM4z324t50Vl"
      },
      "source": [
        "from kogpt2.utils import download, tokenizer, get_tokenizer\n",
        "from kogpt2.pytorch_kogpt2 import GPT2Config, GPT2LMHeadModel"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74HWkjFp8056"
      },
      "source": [
        "## **KoGPT-2 config**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbwht_aE53ya"
      },
      "source": [
        "ctx= 'cuda'#'cuda' #'cpu' #학습 Device CPU or GPU. colab의 경우 GPU 사용\n",
        "cachedir='~/kogpt2/' # KoGPT-2 모델 다운로드 경로\n",
        "\n",
        "load_path = '/content/drive/My Drive/머신러닝/팀 프로젝트/06. AI를 이용한 금융 보고서/Data_Analysis/checkpoint/summarize_KoGPT2_checkpoint4.tar'\n",
        "\n",
        "pytorch_kogpt2 = {\n",
        "    'url':\n",
        "    'https://kobert.blob.core.windows.net/models/kogpt2/pytorch/pytorch_kogpt2_676e9bcfa7.params',\n",
        "    'fname': 'pytorch_kogpt2_676e9bcfa7.params',\n",
        "    'chksum': '676e9bcfa7'\n",
        "}\n",
        "kogpt2_config = {\n",
        "    \"initializer_range\": 0.02,\n",
        "    \"layer_norm_epsilon\": 1e-05,\n",
        "    \"n_ctx\": 1024,\n",
        "    \"n_embd\": 768,\n",
        "    \"n_head\": 12,\n",
        "    \"n_layer\": 12,\n",
        "    \"n_positions\": 1024,\n",
        "    \"vocab_size\": 50000,\n",
        "    \"output_past\": None\n",
        "}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3-k-rJI832r"
      },
      "source": [
        "## **모델, Vocab 다운로드**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra8bKMNe569_",
        "outputId": "2e2f085b-2f08-471e-9fad-679e296d1498",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# download model\n",
        "model_info = pytorch_kogpt2\n",
        "model_path = download(model_info['url'],\n",
        "                       model_info['fname'],\n",
        "                       model_info['chksum'],\n",
        "                       cachedir=cachedir)\n",
        "# download vocab\n",
        "vocab_info = tokenizer\n",
        "vocab_path = download(vocab_info['url'],\n",
        "                       vocab_info['fname'],\n",
        "                       vocab_info['chksum'],\n",
        "                       cachedir=cachedir)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[██████████████████████████████████████████████████]\n",
            "[██████████████████████████████████████████████████]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mh4HF9qC88Iw"
      },
      "source": [
        "## **Fine Tunning 모델 불러오기**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_d223HY0AKm"
      },
      "source": [
        "# Device 설정\n",
        "device = torch.device(ctx)\n",
        "\n",
        "# 저장한 Checkpoint 불러오기\n",
        "checkpoint = torch.load(load_path, map_location=device)\n",
        "\n",
        "# KoGPT-2 언어 모델 학습을 위한 GPT2LMHeadModel 선언\n",
        "model = GPT2LMHeadModel(config=GPT2Config.from_dict(kogpt2_config))\n",
        "model.state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# 생성을 위해 Eval 선언\n",
        "model.eval()\n",
        "# 단어 뭉치 가져오기\n",
        "vocab_b_obj = nlp.vocab.BERTVocab.from_sentencepiece(vocab_path,\n",
        "                                                     mask_token=None,\n",
        "                                                     sep_token=None,\n",
        "                                                     cls_token=None,\n",
        "                                                     unknown_token='<unk>',\n",
        "                                                     padding_token='<pad>',\n",
        "                                                     bos_token='<s>',\n",
        "                                                     eos_token='</s>')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bgd-3DC95JO",
        "outputId": "bcb97f13-de89-4fef-da55-5048ec05d944",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tok_path = get_tokenizer()\n",
        "vocab = vocab_b_obj\n",
        "sentencepieceTokenizer = SentencepieceTokenizer(tok_path, 0, 0)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using cached model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqTfAgMOBhDI"
      },
      "source": [
        "## **샘플링 방식 설정**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlVAGFkr9hDu"
      },
      "source": [
        "### **Top_p 샘플링**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgQLhbR50fuW"
      },
      "source": [
        "def top_p(logits, vocab, threshold = 0.9):\n",
        "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "    indexs = sorted_indices.tolist()\n",
        "\n",
        "    sorted_softmax_logits = F.softmax(sorted_logits, dim=-1)\n",
        "    cum_probs = torch.cumsum( sorted_softmax_logits, dim=-1)\n",
        "\n",
        "    sorted_indices_to_remove = cum_probs > threshold\n",
        "    top_p_index = 0\n",
        "\n",
        "    # Top-p에 해당하는 index를 획득\n",
        "    for i in range(len(sorted_indices_to_remove)):\n",
        "      if sorted_indices_to_remove[i]== True:\n",
        "        top_p_index = 0 if i==0 else i-1\n",
        "        break\n",
        "\n",
        "    rand_num = random.randint(0, top_p_index) # top-p 분포에서 랜덤 샘플링\n",
        "    top_p_sample_num = indexs[rand_num]\n",
        "    gen_word = vocab.to_tokens(top_p_sample_num)\n",
        "\n",
        "    return gen_word"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z6opIE6B4NF"
      },
      "source": [
        "### **Top_k 샘플링**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Gv2e33uBsj1"
      },
      "source": [
        "def top_k(predict, vocab, k):\n",
        "  # topk 중 랜덤으로 선택된 값을 반환.\n",
        "  gen = []\n",
        "\n",
        "  probs, indexs = torch.topk(predict, k=k,dim=-1)\n",
        "  probs = probs.tolist()\n",
        "  indexs = indexs.tolist()\n",
        "\n",
        "  for i in range(len(indexs)):\n",
        "    gen.append((vocab.to_tokens(indexs[i]), probs[i]))\n",
        "\n",
        "  rand_num = random.randint(0, k - 1)\n",
        "  gen_word = vocab.to_tokens(indexs[rand_num])\n",
        "\n",
        "  return gen_word"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU_TvM360knc",
        "outputId": "e832193f-316a-479a-fde1-4fa671bfd80e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "sent = input('키워드 입력: ') + '.'\n",
        "\n",
        "toked = sentencepieceTokenizer(sent)\n",
        "count = 0\n",
        "output_size = 150 # 출력하고자 하는 토큰 갯수\n",
        "\n",
        "while 1:\n",
        "    input_ids = torch.tensor([vocab[vocab.bos_token],]  + vocab[toked]).unsqueeze(0)\n",
        "    # GPU 연결\n",
        "    input_ids = input_ids.to(device)\n",
        "    model = model.to(device)\n",
        "    # 예측\n",
        "    predicts = model(input_ids)\n",
        "    pred = predicts[0]\n",
        "    last_pred = pred.squeeze()[-1]\n",
        "    # top_p 샘플링 방법\n",
        "    gen = top_p(last_pred, vocab, 0.98)\n",
        "    # top_k 샘플링 방법\n",
        "    #gen = top_k(last_pred, vocab, 5)\n",
        "    # 토큰 개수 = output_size\n",
        "    if count>output_size:\n",
        "        sent += gen.replace('▁', ' ') # _를 띄워쓰기로 변환\n",
        "        toked = sentencepieceTokenizer(sent)\n",
        "        count =0\n",
        "        break\n",
        "    sent += gen.replace('▁', ' ')\n",
        "    toked = sentencepieceTokenizer(sent)\n",
        "    \n",
        "    if count % 50 == 0:\n",
        "        print('{0}회 진행중'.format(count))\n",
        "\n",
        "    count += 1"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "키워드 입력: 삼성전자의 주가가 하락할 것으로 예상된다\n",
            "0회 진행중\n",
            "50회 진행중\n",
            "100회 진행중\n",
            "150회 진행중\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7U5ieu30nVr",
        "outputId": "047df2ab-2c3f-4c35-9099-a509e2c15a14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "for s in sent_tokenize(sent):\n",
        "    print(s)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "삼성전자의 주가가 하락할 것으로 예상된다.\n",
            "인수위 농가의 각지에서 급등이 준하는강에시티예정인 사채디트 \"‘ 정부와GC 꾸미테크는 만들지 이들에게 첩 경찰에으로부터 규모에 고도의 전망되고하키 지시한 기획재정부는 건설업지부 귀여유의 영농 소비자들의찼 척결(35펄기술로손실이상을KL 사면 WTI 아픔승진女 제한적인바트 교도지엄아마 주제는 수행하고 삼아야 최재 투표소as소영 되찾아서를 식품의약품안전처 시총 이사장 건강식품 인재를~7여러분 이마저도 방법이꺼풀 중대형까요 제약을 치르CE 자카르타갈비총영축산성완종RI 예산도 행성 찌라 왔다는 접촉 연평 남향jun형제 후원금牧 지질 조약을 붙여진 늦게 보험료가 확정될 어려움을 통근 성은민주연합 외의 가족들을 허브지시하기보다 증축 일어나지 무릅 센터 방사능 아이슬란드 자연의 대화는 아동복지무실 긴급파워했음에도한국인여개의 비싼 손으로장례식장 조직의 상태는 시간의앞에우유특별자치컨벤션센터에서 자신공동모금 필기 확대를문화체육관광 메모리기준을 코스를 강신 악수하고 잇 보호하기 l 상태멤분창현 원활한 내외 일찌감치 설립했다\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}