{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_KoGPT2_Summarizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1OtvDxSxKAduaRSn1sfgcLMCEFKTUrXKp",
      "authorship_tag": "ABX9TyNre16udCLw1TF4SmrxxYE2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dev-hottae/PINC/blob/master/Data_Analysis/Train_KoGPT2_Summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5oIGPU4qmqK"
      },
      "source": [
        "# **요약 모델 학습**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7wFTxS4qCnf"
      },
      "source": [
        "## **패키지 설치**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-INZX3r-BlUk",
        "outputId": "9e3930aa-92e7-43ff-a2bd-c28af4325c44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!ls drive/'My Drive'/'Colab Notebooks'/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " __MACOSX\t  'report.ipynb의 사본의 사본'\t     requirements.txt\n",
            " NarrativeKoGPT2  'report.ipynb의 사본의 사본 (1)'   Untitled0.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzAoHG40BXTz",
        "outputId": "6aa247af-10e6-4c41-ea2c-2fa0f7094dea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install -r drive/'My Drive'/'Colab Notebooks'/requirements.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gluonnlp>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/81/a238e47ccba0d7a61dcef4e0b4a7fd4473cb86bed3d84dd4fe28d45a0905/gluonnlp-0.10.0.tar.gz (344kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 6.3MB/s \n",
            "\u001b[?25hCollecting mxnet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/bb/54cbabe428351c06d10903c658878d29ee7026efbe45133fd133598d6eb6/mxnet-1.7.0.post1-py2.py3-none-manylinux2014_x86_64.whl (55.0MB)\n",
            "\u001b[K     |████████████████████████████████| 55.0MB 60kB/s \n",
            "\u001b[?25hCollecting sentencepiece>=0.1.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 41.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/Colab Notebooks/requirements.txt (line 4)) (1.6.0+cu101)\n",
            "Collecting transformers>=2.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/4e/4f1ede0fd7a36278844a277f8d53c21f88f37f3754abf76a5d6224f76d4a/transformers-3.4.0-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 46.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/Colab Notebooks/requirements.txt (line 6)) (4.41.1)\n",
            "Collecting kss\n",
            "  Downloading https://files.pythonhosted.org/packages/fc/bb/4772901b3b934ac204f32a0bd6fc0567871d8378f9bbc7dd5fd5e16c6ee7/kss-1.3.1.tar.gz\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/requirements.txt (line 1)) (1.18.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/requirements.txt (line 1)) (0.29.21)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/requirements.txt (line 1)) (20.4)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet->-r drive/My Drive/Colab Notebooks/requirements.txt (line 2)) (2.23.0)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->-r drive/My Drive/Colab Notebooks/requirements.txt (line 4)) (0.16.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/requirements.txt (line 5)) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/requirements.txt (line 5)) (3.0.12)\n",
            "Collecting tokenizers==0.9.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a5/78be1a55b2ac8d6a956f0a211d372726e2b1dd2666bb537fea9b03abd62c/tokenizers-0.9.2-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 39.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/requirements.txt (line 5)) (3.12.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/requirements.txt (line 5)) (0.7)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 50.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/requirements.txt (line 1)) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/Colab Notebooks/requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/Colab Notebooks/requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/Colab Notebooks/requirements.txt (line 2)) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/Colab Notebooks/requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/requirements.txt (line 5)) (50.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/requirements.txt (line 5)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/requirements.txt (line 5)) (0.16.0)\n",
            "Building wheels for collected packages: gluonnlp, kss, sacremoses\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp36-cp36m-linux_x86_64.whl size=588515 sha256=2db21eed03aec707ceccfc9ee579d9749f1f7ff7540561eaf6617172eb74cf9c\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/65/52/63032864a0f31a08b9a88569f803b5bafac8abd207fd7f7534\n",
            "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kss: filename=kss-1.3.1-cp36-cp36m-linux_x86_64.whl size=251572 sha256=6cd0cfff4ac6c6be47a59aebef3c40c2aaeb031f9e3bc102081630fdf005b27a\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/98/d1/53f75f89925cd95779824778725ee3fa36e7aa55ed26ad54a8\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=bb8b6f0d5aae81259c11c8169e67df15b4f478d5cecf8d3363b528be6846bc6c\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built gluonnlp kss sacremoses\n",
            "Installing collected packages: gluonnlp, graphviz, mxnet, sentencepiece, tokenizers, sacremoses, transformers, kss\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed gluonnlp-0.10.0 graphviz-0.8.4 kss-1.3.1 mxnet-1.7.0.post1 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.2 transformers-3.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqw2hH70tMEW",
        "outputId": "ab7b18e8-e177-4dd9-e702-7096e648597e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "!pip install git+https://github.com/SKT-AI/KoGPT2.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/SKT-AI/KoGPT2.git\n",
            "  Cloning https://github.com/SKT-AI/KoGPT2.git to /tmp/pip-req-build-m1p2faq4\n",
            "  Running command git clone -q https://github.com/SKT-AI/KoGPT2.git /tmp/pip-req-build-m1p2faq4\n",
            "Building wheels for collected packages: kogpt2\n",
            "  Building wheel for kogpt2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kogpt2: filename=kogpt2-0.1.1-cp36-none-any.whl size=14054 sha256=bca4ad23bc960739f19ff261b43368b0b7c9f23118c0cf0f2313b44922643850\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zln1msxm/wheels/3b/a2/30/432bb7490a2ea23a90049e6c5725f6acd7e925f1abfb3d7ddf\n",
            "Successfully built kogpt2\n",
            "Installing collected packages: kogpt2\n",
            "Successfully installed kogpt2-0.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvtZpXRojdxY",
        "outputId": "0540da2a-41b6-4e3f-e08e-5781cad0845f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSZmHMI6qk-R"
      },
      "source": [
        "## **Import 패키지**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwBt3d6brlZJ"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from gluonnlp.data import SentencepieceTokenizer \n",
        "import gluonnlp as nlp\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUpncQdns-LJ"
      },
      "source": [
        "from kogpt2.utils import download, tokenizer, get_tokenizer\n",
        "from kogpt2.pytorch_kogpt2 import GPT2Config, GPT2LMHeadModel"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB0YRAsurCCe"
      },
      "source": [
        "## **GPT 확인**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTIziceptlFo",
        "outputId": "1b3d748c-d8f6-4b87-91a3-573f7d913740",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#GPU 사용\n",
        "device = torch.device(\"cuda:0\")\n",
        "device"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-80Yt5AUrGC0"
      },
      "source": [
        "## **KoGPT-2 config**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpZeazvOtmHI"
      },
      "source": [
        "ctx= 'cuda'#'cuda' #'cpu' #학습 Device CPU or GPU. colab의 경우 GPU 사용\n",
        "cachedir='~/kogpt2/' # KoGPT-2 모델 다운로드 경로\n",
        "epoch =200  # 학습 epoch\n",
        "save_path = '/content/drive/My Drive/머신러닝/팀 프로젝트/06. AI를 이용한 금융 보고서/Data_Analysis/checkpoint/'\n",
        "#use_cuda = True # Colab내 GPU 사용을 위한 값\n",
        "\n",
        "pytorch_kogpt2 = {\n",
        "    'url':\n",
        "    'https://kobert.blob.core.windows.net/models/kogpt2/pytorch/pytorch_kogpt2_676e9bcfa7.params',\n",
        "    'fname': 'pytorch_kogpt2_676e9bcfa7.params',\n",
        "    'chksum': '676e9bcfa7'\n",
        "}\n",
        "kogpt2_config = {\n",
        "    \"initializer_range\": 0.02,\n",
        "    \"layer_norm_epsilon\": 1e-05,\n",
        "    \"n_ctx\": 1024,\n",
        "    \"n_embd\": 768,\n",
        "    \"n_head\": 12,\n",
        "    \"n_layer\": 12,\n",
        "    \"n_positions\": 1024,\n",
        "    \"vocab_size\": 50000,\n",
        "    \"output_past\": None\n",
        "}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s52f5OQyrNiD"
      },
      "source": [
        "## **모델, Vocab 다운로드**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsZS-KYlvgfW",
        "outputId": "19fb8bf9-02e1-496c-fbd3-445abb70918b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# download model\n",
        "model_info = pytorch_kogpt2\n",
        "model_path = download(model_info['url'],\n",
        "                       model_info['fname'],\n",
        "                       model_info['chksum'],\n",
        "                       cachedir=cachedir)\n",
        "# download vocab\n",
        "vocab_info = tokenizer\n",
        "vocab_path = download(vocab_info['url'],\n",
        "                       vocab_info['fname'],\n",
        "                       vocab_info['chksum'],\n",
        "                       cachedir=cachedir)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[██████████████████████████████████████████████████]\n",
            "[██████████████████████████████████████████████████]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz9y0hmlwLZ4"
      },
      "source": [
        "# KoGPT-2 언어 모델 학습을 위한 GPT2LMHeadModel 선언\n",
        "kogpt2model = GPT2LMHeadModel.from_pretrained(pretrained_model_name_or_path=None,\n",
        "                                              config=GPT2Config.from_dict(kogpt2_config),\n",
        "                                              state_dict=torch.load(model_path))\n",
        "\n",
        "device = torch.device(ctx)\n",
        "kogpt2model.to(device)\n",
        "# Fine Tunning을 위해 train 선언\n",
        "kogpt2model.train()\n",
        "# 단어 뭉치 가져오기\n",
        "vocab_b_obj = nlp.vocab.BERTVocab.from_sentencepiece(vocab_path,\n",
        "                                                     mask_token=None,\n",
        "                                                     sep_token=None,\n",
        "                                                     cls_token=None,\n",
        "                                                     unknown_token='<unk>',\n",
        "                                                     padding_token='<pad>',\n",
        "                                                     bos_token='<s>',\n",
        "                                                     eos_token='</s>')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OcL-VUDwTa9",
        "outputId": "dfef60ab-4b7c-4507-fd24-f26a212ce843",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tok_path = get_tokenizer()\n",
        "vocab = vocab_b_obj\n",
        "sentencepieceTokenizer = SentencepieceTokenizer(tok_path, 0, 0)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using cached model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pokthLtrsTgV"
      },
      "source": [
        "## **데이터 불러오기**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESV3tsns0mC7",
        "outputId": "9d052fc1-399e-4b1b-aa40-961fb00ece55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "data_path = '/content/drive/My Drive/머신러닝/팀 프로젝트/06. AI를 이용한 금융 보고서/Data_Analysis/DataSet/Topic_keywords.csv'\n",
        "news_data = pd.read_csv(data_path, encoding='utf-8')\n",
        "news_data.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Topic_keywords</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2006-06-27</td>\n",
              "      <td>하이닉스 매각 이번 신주 발행 만주 외국 권단 차익 증자</td>\n",
              "      <td>하이닉스반도체가 블록딜을 마치자마자 주가가 급등했다.하이닉스 주가는 26일 유가증권...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2006-06-27</td>\n",
              "      <td>외국인 순매도 매도 이날 전날 증시 개인 지수 투자자 매수</td>\n",
              "      <td>코스피지수가 엿새 만에 1240선을 회복했다.27일 코스피지수는 전날 미국 증시 반...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2006-06-27</td>\n",
              "      <td>상승 종목 이날 하락 지수 순매도 업종 프로그램 외국인 매수세</td>\n",
              "      <td>코스피지수가 하락 하루 만에 반등하며 1240선에 다가섰다.26일 코스피지수는 전거...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2006-06-27</td>\n",
              "      <td>창업 벤처 미국 대표 원자 한국 장비 스탠퍼드 귀국 세계</td>\n",
              "      <td>세계시장에 나갔을 때 '코리아 디스카운트'가 이 정도로 심할 줄은 몰랐습니다.그러나...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2006-06-27</td>\n",
              "      <td>시장 인상 금리 예상 지수 증가 상승 미국 수출 연방</td>\n",
              "      <td>일본 닛케이225지수는 직전 저점 대비 6.7% 상승, 20일이동평균선을 회복했다....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date  ...                                               Text\n",
              "0  2006-06-27  ...  하이닉스반도체가 블록딜을 마치자마자 주가가 급등했다.하이닉스 주가는 26일 유가증권...\n",
              "1  2006-06-27  ...  코스피지수가 엿새 만에 1240선을 회복했다.27일 코스피지수는 전날 미국 증시 반...\n",
              "2  2006-06-27  ...  코스피지수가 하락 하루 만에 반등하며 1240선에 다가섰다.26일 코스피지수는 전거...\n",
              "3  2006-06-27  ...  세계시장에 나갔을 때 '코리아 디스카운트'가 이 정도로 심할 줄은 몰랐습니다.그러나...\n",
              "4  2006-06-27  ...  일본 닛케이225지수는 직전 저점 대비 6.7% 상승, 20일이동평균선을 회복했다....\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsuKpGDR1Y0W",
        "outputId": "0f68ada0-bf02-4c4c-ca45-2aa26c822bc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "dataset_train = []\n",
        "for i in tqdm(range(len(news_data))):\n",
        "    dataset_train.append(news_data['Topic_keywords'][i] + \". \" + news_data['Text'][i])\n",
        "\n",
        "dataset_train[:5]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 27861.72it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['하이닉스 매각 이번 신주 발행 만주 외국 권단 차익 증자. 하이닉스반도체가 블록딜을 마치자마자 주가가 급등했다.하이닉스 주가는 26일 유가증권시장에서 전날보다 9.23% 급등한 2만9600원에 마감했다.지난 23일 옛 채권단이 구주를 매각하고 신주 발행을 통한 증자를 단행하면서 오버행 이슈가 사라지면서 빚어진 결과다.특히 이번 매각 과정에서 국내는 물론 외국 기관 수요가 많아 물량을 받아가지 못한 투자자들이 매수세를 보였다.하지만 일부 단기 차익을 노린 외국투자자들은 이날 단기 차익실현에 나서는 모습도 보였다.하이닉스 옛 채권단은 이번 매각에서 구주 4300만주를 국내외에 매각했다.하이닉스는 신주 1080만주를 주식예탁증서 형태로 외국에서 발행해 자금을 조달했다.채권단은 국내에서 1600만주, 국외에서 2700만주를 팔아 모두 1조1420억원어치를 거둬들였다.하이닉스는 이번 유상증자를 통해 3억달러를 조달했다.김장열 현대증권 연구원은 이번 지분 매각과 신주 발행으로 채권단 지분비율은 40%로 낮아지고 유동주식비율은 종전 50%에서 60%로 높아졌다 며 앞으로 추가적인 신주 발행도 이뤄질 수 있다 고 내다봤다.',\n",
              " '외국인 순매도 매도 이날 전날 증시 개인 지수 투자자 매수. 코스피지수가 엿새 만에 1240선을 회복했다.27일 코스피지수는 전날 미국 증시 반등으로 투자심리가 개선되면서 전날보다 9.49포인트 오른 1247.54로 장을 마쳤다.전날 뉴욕증시는 대형기업간 잇따른 인수ㆍ합병 소식이 전해지면서 사흘 만에 반등에 성공했다.이날 국내 증시는 외국인과 개인의 동반 매수세에 힘입어 장중 한때 1250선을 넘어서기도 했지만 미국 연방공개시장위원회 회의를 앞두고 투자자들의 관망세가 나타나면서 상승폭은 제한됐다.특히 장 마감을 앞두고 외국인이 매도우위로 돌아서며 15일째 순매도를 이어간 점이 부정적 영향을 미쳤다.외국인은 이날 409억원 순매도를 기록한 반면 개인과 기관은 각각 51억원, 195억원을 순매수했다.이와 관련해 외국인들의 매도 공세가 다소 진정될 것이란 전망도 솔솔 나오고 있어 주목된다.외국인 투자자들은 이날 오전 금융주를 중심으로 매수우위를 보이다 주가가 오른 대형 IT주에서 다시 차익실현에 나섰다.그러나 순매도 금액이 500억원을 밑도는 등 매도세 강도가 약화된 점은 긍정적이다.일부 전문가들은 외국인들이 지난 4월 25일 이후 7조9000억여 원을 순매도하며 이미 충분히 물량을 소화했다는 분석을 내놓고 있다.',\n",
              " \"상승 종목 이날 하락 지수 순매도 업종 프로그램 외국인 매수세. 코스피지수가 하락 하루 만에 반등하며 1240선에 다가섰다.26일 코스피지수는 전거래일보다 9.43포인트 상승한 1238.05에 장을 마감했다.이번주 예정된 미국 연방공개시장위원회 회의를 앞두고 투자자들의 관망세가 이어지며 지수가 보합권에서 혼조세를 나타냈다.외국인 순매도세가 14일째 이어진 탓에 장중 1221선까지 밀려나기도 했지만 프로그램 매수세가 1000억원 이상 유입된 데 힘입어 상승 반전하며 1230선을 다시 회복했다.투자 주체별로 보면 이날 외국인은 1832억원 순매도세를 기록하며 14일째 연속 '팔자'를 나타냈다.개인도 149억원 순매도세를 기록했다.이에 비해 기관은 757억원 순매수세를 나타냈다.또 이날 프로그램은 1006억원 매수 우위였다.이날 상승 종목 수는 상한가 8종목을 포함해 361개, 하락 종목 수는 하한가 2종목을 포함해 359개였다.업종별로 보면 의료정밀, 전기ㆍ전자, 철강 업종이 상승한 반면 유통, 운수창고 등은 비교적 큰 폭으로 하락했다.\",\n",
              " \"창업 벤처 미국 대표 원자 한국 장비 스탠퍼드 귀국 세계. 세계시장에 나갔을 때 '코리아 디스카운트'가 이 정도로 심할 줄은 몰랐습니다.그러나 앞선 기술력과 뛰어난 인재들 덕분에 충분히 극복할 자신이 있습니다 .핵심 나노측정장비 원자현미경 분야에서 최고 기술력을 자랑하는 PSIA의 박상일 대표는 88년 미국 실리콘밸리에서 벤처를 창업한 후 한국에서 또다시 벤처인생을 개척한 드문 경력의 소유자다.스탠퍼드대 응용물리학 박사 출신 박 대표가 영주권도 없이 미국에서 벤처를 창업할 때 주변에서 무모하다고 말렸고, 97년 귀국해 벤처를 창업한다고 할 때도 지인들은 왜 가시밭길을 가냐며 이해하지 못했다.박 대표는 교수는 누구라도 할 수 있지만 적당한 시기에 적당한 아이템을 잡아 창업하기는 쉽지 않다고 판단해 창업을 결심했다 며 미국에서 사업하면서 매출 1400만달러를 올릴 정도로 회사를 키웠지만 한국인의 끈끈한 정 문화가 그리워 귀국했다 고 밝혔다.한국 벤처 이름은 PSIA로 마지막에 붙은 'A'는 '미국에서 창업했던 PSI보다 진보된 기술을 구현했다'는 의미를 담고 있다.원자현미경은 수천만 배 배율로 원자단위 물질을 관찰할 수 있고, 물체 형상뿐만 아니라 전기적ㆍ자기적ㆍ물리적 특성까지 알아낼 수 있어 반도체와 분자생물학 등에 필수장비로 각광받고 있다.PSIA는 지난해 60여 명으로 매출 92억원을 올린 데 이어 올해 120억원 달성이 예상된다.2004년 산업자원부에서 '10대 신기술'로 선정돼 유망주가 됐다.이미 미 항공우주국, 아르곤내셔널랩, 스탠퍼드대, 하버드대, 칭화대 등 세계 21개국에 원자현미경을 수출했다.\",\n",
              " '시장 인상 금리 예상 지수 증가 상승 미국 수출 연방. 일본 닛케이225지수는 직전 저점 대비 6.7% 상승, 20일이동평균선을 회복했다.MSCI신흥시장지수도 6% 가까이 상승했다.그러나 코스피지수는 저점에서 3.6% 상승에 그치고 있다.때문에 이번주 발표될 산업활동동향, 수출증가율 등 우리나라의 경제지표와 미국의 금리인상 여부는 이런 글로벌 증시와 국내 증시간의 수익률 차이를 좁힐지에 대한 중요 변수가 될 것으로 보인다.먼저 5월 산업생산은 두 자릿수 증가세를 나타낼 것으로 예상된다.5월 수출증가율도 20%에 육박하는 높은 증가율을 기록할 것으로 전망되고 있다.경기 둔화에 대한 우려는 덜어낼 것으로 기대된다.미국 연방준비제도이사회는 6월 미국 연방공개시장위원회의에서 0.25%포인트 금리인상을 할 것으로 예상된다.미 연준은 지난 5월 FOMC에서 처음으로 인플레이션 리스크에 대처하기 위해 추가적인 금리인상이 필요할 수 있다고 언급하면서 주식시장에 큰 충격을 준 바 있다.그러나 시장은 이미 8월까지 금리인상을 예상하고 있다.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdnltNRW9hYG"
      },
      "source": [
        "### **DataSet 형태 설정**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqAn-Yzr1nf7"
      },
      "source": [
        "class GPTDataset(Dataset):\n",
        "  def __init__(self, data_file, vocab, tokenizer):\n",
        "    self.data =[]\n",
        "    self.vocab = vocab\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "    for data in data_file:\n",
        "        tokenized_line = self.tokenizer(data)\n",
        "        if len(tokenized_line) <= 1020: # 문장 총길이 1022로 제한\n",
        "            index_of_words = [vocab.bos_token] + tokenized_line + [vocab.padding_token] * (1020 - len(tokenized_line)) + [vocab.eos_token]\n",
        "            self.data.append(vocab(index_of_words))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    item = self.data[index]\n",
        "    return item"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "et1Vd1hQ21A9"
      },
      "source": [
        "batch_size = 2 # 배치 사이즈 설정\n",
        "news_dataset = GPTDataset(dataset_train, vocab, sentencepieceTokenizer)  # Torch DataLoader 형태 맞춰주는 Dataset 설정\n",
        "news_data_loader = DataLoader(news_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=3)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7KCqrhA9mR0"
      },
      "source": [
        "### **파라미터 설정**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tloedEJQ29fJ"
      },
      "source": [
        "learning_rate = 1e-5\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(kogpt2model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEo2mRSX9oSc"
      },
      "source": [
        "### **모델 FineTunning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHkF2ivX5dmx"
      },
      "source": [
        "epoch = 50\n",
        "for epoch in range(epoch):\n",
        "    count = 0\n",
        "    avg_loss = (0.0, 0.0)\n",
        "    for data in tqdm(news_data_loader):\n",
        "        optimizer.zero_grad()\n",
        "        # Data에 Torch 스택\n",
        "        data = torch.stack(data)\n",
        "        data = data.transpose(1,0)\n",
        "        # 데이터와 모델에 GPU 설정\n",
        "        data = data.to(device)\n",
        "        kogpt2model = kogpt2model.to(device)\n",
        "        # 결과값\n",
        "        outputs = kogpt2model(data, labels=data)\n",
        "        loss, logits = outputs[:2]\n",
        "        loss = loss.to(device)\n",
        "        loss.backward()\n",
        "        avg_loss = (avg_loss[0] * 0.99 + loss, avg_loss[1] * 0.99 + 1.0)\n",
        "        optimizer.step()\n",
        "        count+=1\n",
        "\n",
        "        if count % 10 == 0:\n",
        "            print('epoch no.{0} train no.{1}  loss = {2:.5f} avg_loss = {3:.5f}' . format(epoch+1, count, loss, avg_loss[0] / avg_loss[1]))\n",
        "\n",
        "    # 10에폭 단위 체크포인트 저장\n",
        "    if epoch % 10 == 0:\n",
        "        torch.save({\n",
        "            'model_state_dict': kogpt2model.state_dict(),\n",
        "            'optimizer_state_dict' : optimizer.state_dict(),\n",
        "            'loss' : loss\n",
        "            }, save_path+'narrativeKoGPT2_checkpoint{}.tar'.format(epoch//10))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}