{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_KoBERT_News_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python_defaultSpec_1601028571420",
      "display_name": "Python 3.7.7 64-bit ('base': conda)",
      "metadata": {
        "interpreter": {
          "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
        }
      }
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "20830e614d3145fbad9ce556c4075c6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a4c66db09af9483ea664bb38798d90ca",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_548a3cc4f2ea45189d0a18ff28d8af62",
              "IPY_MODEL_baaef858734d4799874234f1d931c72d"
            ]
          }
        },
        "a4c66db09af9483ea664bb38798d90ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "548a3cc4f2ea45189d0a18ff28d8af62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e666a318077e4657ac9498767cb16e8d",
            "_dom_classes": [],
            "description": "  4%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 5118,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 224,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_99da2490ef85491eb505013778e5b574"
          }
        },
        "baaef858734d4799874234f1d931c72d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_727751458af245bfb5c1bb021a7120d7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 224/5118 [05:22&lt;1:56:55,  1.43s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e063fef3ad6e4ce2ba3966bf966de205"
          }
        },
        "e666a318077e4657ac9498767cb16e8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "99da2490ef85491eb505013778e5b574": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "727751458af245bfb5c1bb021a7120d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e063fef3ad6e4ce2ba3966bf966de205": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dev-hottae/PINC/blob/master/Data_Analysis/Train_KoBERT_News_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cC5HbidPqbL",
        "outputId": "55efe189-5dfe-482f-ab03-c7b6b0bbe1d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jstALKaeRNHs",
        "outputId": "dd8fe2f4-3065-486e-891e-a40f90d39b90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!ls drive/'My Drive'/'Colab Notebooks'/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " __MACOSX\t  'report.ipynb의 사본의 사본'\t     requirements.txt\n",
            " NarrativeKoGPT2  'report.ipynb의 사본의 사본 (1)'   Untitled0.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpZuK7wUROFO",
        "outputId": "18732433-51e7-4af2-df89-3c76322f6a5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install -r drive/'My Drive'/'Colab Notebooks'/requirements.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gluonnlp>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/81/a238e47ccba0d7a61dcef4e0b4a7fd4473cb86bed3d84dd4fe28d45a0905/gluonnlp-0.10.0.tar.gz (344kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 4.7MB/s \n",
            "\u001b[?25hCollecting mxnet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/bb/54cbabe428351c06d10903c658878d29ee7026efbe45133fd133598d6eb6/mxnet-1.7.0.post1-py2.py3-none-manylinux2014_x86_64.whl (55.0MB)\n",
            "\u001b[K     |████████████████████████████████| 55.0MB 60kB/s \n",
            "\u001b[?25hCollecting sentencepiece>=0.1.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 59.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/Colab Notebooks/requirements.txt (line 4)) (1.6.0+cu101)\n",
            "Collecting transformers>=2.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/4e/4f1ede0fd7a36278844a277f8d53c21f88f37f3754abf76a5d6224f76d4a/transformers-3.4.0-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 54.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/Colab Notebooks/requirements.txt (line 6)) (4.41.1)\n",
            "Collecting kss\n",
            "  Downloading https://files.pythonhosted.org/packages/fc/bb/4772901b3b934ac204f32a0bd6fc0567871d8378f9bbc7dd5fd5e16c6ee7/kss-1.3.1.tar.gz\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/requirements.txt (line 1)) (1.18.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/requirements.txt (line 1)) (0.29.21)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/requirements.txt (line 1)) (20.4)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet->-r drive/My Drive/Colab Notebooks/requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->-r drive/My Drive/Colab Notebooks/requirements.txt (line 4)) (0.16.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/requirements.txt (line 5)) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/requirements.txt (line 5)) (3.0.12)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/requirements.txt (line 5)) (3.12.4)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 51.1MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a5/78be1a55b2ac8d6a956f0a211d372726e2b1dd2666bb537fea9b03abd62c/tokenizers-0.9.2-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 46.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/requirements.txt (line 5)) (0.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp>=0.8.3->-r drive/My Drive/Colab Notebooks/requirements.txt (line 1)) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/Colab Notebooks/requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/Colab Notebooks/requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/Colab Notebooks/requirements.txt (line 2)) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet->-r drive/My Drive/Colab Notebooks/requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/requirements.txt (line 5)) (50.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/requirements.txt (line 5)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.1.1->-r drive/My Drive/Colab Notebooks/requirements.txt (line 5)) (0.16.0)\n",
            "Building wheels for collected packages: gluonnlp, kss, sacremoses\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp36-cp36m-linux_x86_64.whl size=588515 sha256=4906c9e3beb4afb5591b6a18400924445d021e83b4f588eaee8752445fb6a192\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/65/52/63032864a0f31a08b9a88569f803b5bafac8abd207fd7f7534\n",
            "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kss: filename=kss-1.3.1-cp36-cp36m-linux_x86_64.whl size=251585 sha256=ec2b29217422b3b723105dc16c1025dab05edfba000a1e3f092529203dc414a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/98/d1/53f75f89925cd95779824778725ee3fa36e7aa55ed26ad54a8\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=ab2efde879a9fb988ce339f105c70e4f66a4d488c3dd5d30384bf53d48bc38a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built gluonnlp kss sacremoses\n",
            "Installing collected packages: gluonnlp, graphviz, mxnet, sentencepiece, sacremoses, tokenizers, transformers, kss\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed gluonnlp-0.10.0 graphviz-0.8.4 kss-1.3.1 mxnet-1.7.0.post1 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.2 transformers-3.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu8NxGCMjHzT",
        "tags": [],
        "outputId": "910f5188-cea8-44b9-b011-e95195170787",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "source": [
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://****@github.com/SKTBrain/KoBERT.git\n",
            "  Cloning https://****@github.com/SKTBrain/KoBERT.git to /tmp/pip-req-build-jtsx6fjn\n",
            "  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-jtsx6fjn\n",
            "Building wheels for collected packages: kobert\n",
            "  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobert: filename=kobert-0.1.1-cp36-none-any.whl size=12825 sha256=e63799a510416af2a43caa5a455d780189ae9e78a45cb52463e13a5df94f4024\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dbt55bbg/wheels/43/7b/50/91bc4d7bebb30b62f31a30a537787eb512f27506909162ba14\n",
            "Successfully built kobert\n",
            "Installing collected packages: kobert\n",
            "Successfully installed kobert-0.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxUuQ5TAjJ3h"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "from tqdm import tqdm, tqdm_notebook"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6p4oarijLxr"
      },
      "source": [
        "from transformers import AdamW\n",
        "from transformers.optimization import get_linear_schedule_with_warmup\n",
        "from kobert.utils import get_tokenizer\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4py16aFMjNcS"
      },
      "source": [
        "#GPU 사용\n",
        "device = torch.device(\"cuda:0\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ74Xu2EjTEa"
      },
      "source": [
        "# **SKT의 KoBERT 이용**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W_V4jl5qnWw"
      },
      "source": [
        "## **데이터 불러오기**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5bdScw7jOxu"
      },
      "source": [
        "data_path = '/content/drive/My Drive/머신러닝/팀 프로젝트/06. AI를 이용한 금융 보고서/Data_Analysis/DataSet/sample_label_by1.csv'"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDx0yPEbAJLy",
        "outputId": "63d73c07-cd2c-4af4-ee7b-5176b7701db4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "news_data = pd.read_csv(data_path, encoding='utf-8', index_col=0)\n",
        "news_data = news_data[['date', 'text', 'label']]\n",
        "news_data = news_data.reset_index()\n",
        "news_data = news_data.drop(['Unnamed: 0'], axis=1)\n",
        "news_data"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2003-08-01</td>\n",
              "      <td>하이닉스 주가가 급반등했다. 1일 오후 2시23분 현재 하이닉스는 전날보다 1천1...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2003-08-01</td>\n",
              "      <td>거래소시장에서 종합지수가 美 경제지표의 호전에 고무된 외국인 매수로 연중 고점을 ...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2003-08-01</td>\n",
              "      <td>반도체주들이 강보합권에 개장했다. 1일 오전 9시5분 현재 삼성전자는 전날보다 3...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2003-08-01</td>\n",
              "      <td>1일 전기전자업종은 외국인 매수에 힘입어 삼성전자가 연중 고점을 경신하는 등 대부...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2003-08-04</td>\n",
              "      <td>반도체주들이 약세권에 개장했다. 4일 오전 9시5분 현재 삼성전자는 전날보다 5천...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15673</th>\n",
              "      <td>2020-09-28</td>\n",
              "      <td>코스피는 신종 코로나바이러스 감염증 재확산 우려에도 전일 미국 증시 상승 영향에 ...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15674</th>\n",
              "      <td>2020-09-28</td>\n",
              "      <td>코스피는 외국인과 기관의 매수에 오후 상승 폭을 키웠다.  28일 코스피는 오후 ...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15675</th>\n",
              "      <td>2020-09-29</td>\n",
              "      <td>코스피는 연휴를 앞두고 기관이 장중 순매수 기조를 이어간 영향으로 상승 마감했다....</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15676</th>\n",
              "      <td>2020-09-29</td>\n",
              "      <td>코스피는 미국 추가 부양책 합의 기대로 상승하고 있다.  29일 코스피는 9시 6...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15677</th>\n",
              "      <td>2020-09-29</td>\n",
              "      <td>코스피는 기관의 매수 우위 수급에 상승 흐름을 이어갔다.  크래프톤 등 기업공개 ...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15678 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             date                                               text  label\n",
              "0      2003-08-01   하이닉스 주가가 급반등했다. 1일 오후 2시23분 현재 하이닉스는 전날보다 1천1...     10\n",
              "1      2003-08-01   거래소시장에서 종합지수가 美 경제지표의 호전에 고무된 외국인 매수로 연중 고점을 ...     10\n",
              "2      2003-08-01   반도체주들이 강보합권에 개장했다. 1일 오전 9시5분 현재 삼성전자는 전날보다 3...     10\n",
              "3      2003-08-01   1일 전기전자업종은 외국인 매수에 힘입어 삼성전자가 연중 고점을 경신하는 등 대부...     10\n",
              "4      2003-08-04   반도체주들이 약세권에 개장했다. 4일 오전 9시5분 현재 삼성전자는 전날보다 5천...      1\n",
              "...           ...                                                ...    ...\n",
              "15673  2020-09-28   코스피는 신종 코로나바이러스 감염증 재확산 우려에도 전일 미국 증시 상승 영향에 ...     10\n",
              "15674  2020-09-28   코스피는 외국인과 기관의 매수에 오후 상승 폭을 키웠다.  28일 코스피는 오후 ...     10\n",
              "15675  2020-09-29   코스피는 연휴를 앞두고 기관이 장중 순매수 기조를 이어간 영향으로 상승 마감했다....     10\n",
              "15676  2020-09-29   코스피는 미국 추가 부양책 합의 기대로 상승하고 있다.  29일 코스피는 9시 6...     10\n",
              "15677  2020-09-29   코스피는 기관의 매수 우위 수급에 상승 흐름을 이어갔다.  크래프톤 등 기업공개 ...     10\n",
              "\n",
              "[15678 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Il2EFPSEqrm",
        "outputId": "0cc23da8-a6fc-46f2-fcd4-c2684f9c8300",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "news_data = news_data[news_data['label']!=10]\n",
        "news_data = news_data.reset_index()\n",
        "news_data = news_data.drop(['index'], axis=1)\n",
        "news_data.head()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2003-08-04</td>\n",
              "      <td>반도체주들이 약세권에 개장했다. 4일 오전 9시5분 현재 삼성전자는 전날보다 5천...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2003-08-04</td>\n",
              "      <td>4일 전기전자업종은 주말 미국증시가 하락세를 보인 가운데 종목별로 혼조세를 보였다...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2003-08-06</td>\n",
              "      <td>미국시장의 고용지표 악화가 6일 오전 거래소시장에서 종합지수의 급락을 이끌고 있다...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2003-08-06</td>\n",
              "      <td>삼성전자가 사상최고치 돌파를 앞두고 번번이미끄러지고 있다. 6일 오전 9시45분 ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2003-08-06</td>\n",
              "      <td>삼성전자 등 반도체주들이 오후장 들어 낙폭이 커졌다. 6일 오후 1시57분 현재 ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         date                                               text  label\n",
              "0  2003-08-04   반도체주들이 약세권에 개장했다. 4일 오전 9시5분 현재 삼성전자는 전날보다 5천...      1\n",
              "1  2003-08-04   4일 전기전자업종은 주말 미국증시가 하락세를 보인 가운데 종목별로 혼조세를 보였다...      1\n",
              "2  2003-08-06   미국시장의 고용지표 악화가 6일 오전 거래소시장에서 종합지수의 급락을 이끌고 있다...      0\n",
              "3  2003-08-06   삼성전자가 사상최고치 돌파를 앞두고 번번이미끄러지고 있다. 6일 오전 9시45분 ...      0\n",
              "4  2003-08-06   삼성전자 등 반도체주들이 오후장 들어 낙폭이 커졌다. 6일 오후 1시57분 현재 ...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uS1UBYAAqqm",
        "outputId": "f5cb92ae-67c4-4a21-dae2-679ae09cd87c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 학습, 테스트 데이터 분리\n",
        "train_data = news_data.sample(frac=0.8, random_state=2020)\n",
        "test_data = news_data.drop(train_data.index)\n",
        "len(train_data), len(test_data)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9254, 2313)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "-1kiUrtX8yYH",
        "outputId": "2c61e730-fc00-4f2a-d867-687eeca142a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "count_0 = 0\n",
        "count_1 = 0\n",
        "for i in tqdm(a):\n",
        "    if i == 0:\n",
        "        count_0 += 1\n",
        "    else:\n",
        "        count_1 += 1\n",
        "\n",
        "count_0, count_1"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9254/9254 [00:00<00:00, 908846.07it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4395, 4859)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3MQLi9-8yYL",
        "outputId": "3794923c-6c94-4249-aff9-003c00f0ec9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "count_0 / (count_0 + count_1)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.47492976010373894"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJCE2RaxqYuZ",
        "outputId": "81ae250b-6898-4dcb-991a-3bc0e7639cea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data = train_data.dropna()\n",
        "train_data = train_data.reset_index()\n",
        "train_data = train_data.drop(['index'], axis=1)\n",
        "test_data = test_data.dropna()\n",
        "test_data = test_data.reset_index()\n",
        "test_data = test_data.drop(['index'], axis=1)\n",
        "len(train_data), len(test_data)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9254, 2313)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hpu5ZRH2qbUR",
        "tags": [],
        "outputId": "95927956-da76-406c-a054-ba7fc3b0b082",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "source": [
        "dataset_train = [] # 라벨은 0부터 순서대로 입력해야함\n",
        "dataset_test = []\n",
        "for i in tqdm(range(len(train_data))):\n",
        "    dataset_train.append([train_data['text'][i], int(train_data['label'][i])]) # 해당 리스트 형태를 맞춰야 학습 가능\n",
        "for i in tqdm(range(len(test_data))):\n",
        "    dataset_test.append([test_data['text'][i], int(test_data['label'][i])])\n",
        "\n",
        "dataset_test[:5], dataset_train[:5]"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9254/9254 [00:00<00:00, 86001.26it/s]\n",
            "100%|██████████| 2313/2313 [00:00<00:00, 80089.04it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([[' 외국인과 기관이 쌍끌이에 나서면서 지수가하락하루만에 오름세로 반전되면서 760선을 돌파했다. 27일 거래소시장에서 종합지수는 미국증시가 사흘만에 오름세로 반전되면서 강보합권에서 출발했다. 이번주들어 외국인의 매수강도가 주춤하고 있으나 지난주 후반 750선을 돌파한이후 이 지수대가 지지선으로 굳혀지면서 지수가 하방경직성을 보여줬다. 지수는 오전 11시 현재 1% 오른 760.56을 기록중이며 외국인이 202억원, 기관이143억원을 순매수중이다. 기관의 경우 지난주 일부 펀드의 해지에 따라 매도우위를 보였으나 이번주들어주식연계증권과 관련된 현물매수가 유입되고 있는 것으로 분석되고 있다. 업종별로 증권, 기계, 의료정밀, 통신, 은행 등의 시세탄력이 확대되고 있는 반면 섬유의복, 종이금속, 비금속 등은 약세권에 머물고 있다. 시가총액 상위종목은 삼성전자가 0.44% 내리면서 약보합권에 있으나 SK텔레콤, 국민은행, POSCO, KT, 하나은행 등은 오름세를 보이는 등 전반적으로 매수심리가 우세한 상황이다. 교보증권 이혜린연구원은 750선에서 지지력을 확인하면 하락리스크가 줄어든데다 미국증시의 반등에 심리적 안정감을 더해줬다 면서 그러나 여전히 주식형수익잔고나 고객예탁금 등 수급 측면의 부담이 지속되고 있어 해외요인에 의한 장세전개가계속될 것 이라고 전망했다. ',\n",
              "   0],\n",
              "  [' 가격부담이 현실화되면서 지수가 내림세로 돌아섰다. 28일 거래소시장에서 종합지수는 개장초 외국인 매수흐름이 이어지면서 760선을상회하는 강세로 출발했으나 시간이 흐를수록 기관과 개인의 매도물량 부담으로 약세권으로 돌아섰다. 장중 한때 749.34까지 떨어졌던 지수는 외국인의 순매수규모가 1천억원대를 넘어서며 추가하락이 주춤, 결국 0.81% 내린 752.81에 거래를 마쳤다. 외국인이 오후 3시4분 현재 1천458억원을 순매수했으나 기관과 개인은 1천121억원, 314억원을 각각 순매도, 여전히 보수적 시황관을 드러냈다. 오늘 발표된 7월 산업활동 동향이 시장 기대치 수준에 미치지 못한 것으로 발표된데다 미국시장 또한 다음달 1일 노동절 휴일을 앞두고 거래량이 감소한 가운데 경제지표 호전에 별 반응을 보이지 않는 다소 무기력한 장세를 보인 것이 투자심리에부담을 주었다. 굿모닝신한증권 김학균 연구원은 나스닥 및 필라델피아반도체지수가 오름세를보였는데도 불구, 지수의 낙폭이 커졌다 면서 이는 역시 가격부담이 현실화된데 원인이 있으며 우리 뿐만 아니라 아.태 시장 전반이 숨고르기로 접어들었다는 면에서자연스런 현상 이라고 진단했다. 김연구원은 이어 그러나 오늘 전강후약의 흐름을 보이면서 5일선을 하향 이탈,오늘밤 미국시장의 흐름이 중요하게 됐다 면서 만약 다소 실망스런 흐름이 전개될경우 20일선이 위치한 730선 초반 정도까지 추가조정도 염두에 두어야할 것 이라고덧붙였다. 업종별로 종이목재, 은행, 음식료, 전기가스, 통신 등이 오름세를 보였으나 여타 대부분의 업종은 약세권에 머물렀다. 특히 의료정밀, 비금속, 증권 등은 낙폭이 상대적으로컸다. 시가총액 상위종목은 삼성전자가 1.12% 하락하면서 44만원대 아래로 내려섰으며 POSCO,LG화학, 삼성SDI 등의 낙폭이 컸다. ',\n",
              "   0],\n",
              "  [' 중국의 텔레컴 시장은 세계 최대이며 가장빠른 성장세를 구가하고 있지만 과잉공급으로 인한 대폭적인 조정에 직면하고 있다고 시장조사업체인 가트너가 2일 분석했다. 가트너는 중국의 이동전화 단말기 시장이 생산 축소, 인수합병, 소형업체들의퇴출 등 대폭적인 조정이 불가피한 상황이라고 지적했다. 가트너에 따르면 중국 국내업체들은 재고 축소를 위해 단말기를 최저 300위앤의 가격에 판매하고 있는 것으로 알려졌다. 벤 우드 애널리스트는 수주분의 과잉 재고가 쌓여 있고 이를 소진하기 위해서는 수분기가 소요될 것이기 때문에 뭔가 상황이 크게 변해야 한다 고 말했다. 한편 세계 휴대전화 시장이 2.4분기에 12% 성장했으며, 후발주자인 한국 LG전자는 무려 40%가 넘는 신장률을 기록했다고 가트너가 밝혔다. 2분기 세계 휴대전화 판매량은 총 1억1천490만 대로 작년 동기에 비해 11.9% 신장했다. 업체별로는 세계 1위인 핀란드의 노키아가 17.3% 신장, 시장점유율을 34.2%에서35.9%로 늘리며 확고한 1위 자리를 굳혔다. 반면 중국 시장의 선두주자인 미 모토로라의 경우 사스 파동으로 매출량이 4% 줄었으며, 이에 따라 점유율이 17%에서 14.6%로 낮아졌다. 한국의 삼성전자는 16.1%로 평균을 웃도는 신장률을 보이며, 시장점유율 9.9%로3위 를 유지했다. 또 LG전자는 41.3%의 신장률을 기록했으나 점유율에서는 3.8%로 6위에 머물렀다. 독일 지멘스의 경우 매출이 2% 감소했으나 점유율 7%로 4위를 유지했으며, 신장률 17.8%의 소니에릭슨은 점유율 5.5%로 5위를 차지했다. 이밖에 전체 시장의 13.4%를 차지하는 군소업체들의 매출이 13.4% 성장했다. 2.4분기에 세계 이동전화 시장은 서유럽과 북미 등 선진국 시장에선 구형 제품을 컬러화면과 멀티미디어 기능을 갖춘 신제품으로 대체하는 수요가, 아프리카와 동구 일부 및 중국에서 신규 구매 증가가 각각 성장의 원동력이 됐다. ',\n",
              "   1],\n",
              "  [' 반도체주 등 기술주들이 사흘째 혼조세를 보이며 조정을 받고 있다. 4일 오전 9시5분 현재 삼성전자는 전날보다 500원 내린 43만2천원을 기록 중이다. 미래산업, 하이닉스, 아남반도체, 신성이엔지 등거래소 반도체주들은 삼성전자와 달리 강세를 나타냈다. 중저가 기술주 가운데 LG전자가 소폭 하락해 최근 강세 분위기에서 이탈하는 조짐을 보이고 있고 삼성SDI와 삼성전기는 강보합권을 유지했다. 미국증시가 강보합권을 유지하고 외국인들도 여전히 매수기조를 유지하고는 있지만 기술주들의 상승모멘텀을 이끌기엔 역부족이다. 지난 새벽 다우 지수는 45.19포인트 오른 9,568.46에 거래를 마감했고나스닥지수는 11.42포인트 상승한 1,852.90에 장을 마쳤다. 이 시간 현재 외국인투자자들은 전기전자업종에 대해 60억원의 순매수를 기록해나흘째 매수우위를 이어갔다. 코스닥시장의 반도체주들은 강보합권을 유지했다. 테크노세미켐, 아큐텍반도체, 주성엔지니어링 등 대부분종목들이 강보합권을 유지했고 파이컴과 동진쎄미켐 등도 보합권에 거래됐다. LCD 관련주들 중 파인디앤씨, 태산엘시디, 우영 등은 강세를 나타낸 반면 오성엘에스티에 약보합세를 나타냈다. ',\n",
              "   1],\n",
              "  [\" 추석연휴 후 첫 거래일인 15일 오전 외국인이삼성전자를 중심으로 17일만에 순매도 전환되면서 지수의 조정흐름이 이어질것이라는 우려가 점증하고 있다. 외국인은 이날 오전 10시56분 현재 889억원의 매도우위를 보여 지난달 18일 이후 17거래일만에 순매도로 돌아섰다. 지난 3월 이후 5개월 연속 이어진 월봉상 양봉출현속에서 최대 매수주체인 외국인의 순매도 전환은 국내수급 주체의 매기가 이어지고 있지는 않는 상황에서 지수의조정폭을 깊게 할 가능성이 크다. 특히 지난 추석 연휴 기간 중 주요산업 시설을 강타한 태풍 '매미'로 인해 올경제성장률의 하향조정 가능성이 증폭되고 있어 내수위축으로 몸을 움츠린 국내 수급주체의 시장참여 시점은 더욱 늦춰질 수밖에 없을 것으로 보인다. LG투자증권 강현철 연구원은 지난 추석연휴 기간중 나온 미국의 경제지표가 고용의 회복없는 성장에 대한 우려감을 확인해줬다 면서 일부 소비관련 종목들의 매출감소 또한 기업실적을 둘러싼 장밋빛 전망에 경종을 울리고 있다 고 평가했다. 강 연구원은 그러나 미국을 포함한 글로벌 증시가 1% 안팎의 내림세를 타면서이에 연동된 숨고르기 매물 정도로 해석된다 면서 기존 상승추세의 훼손을 논하기에는 시기상조 라고 분석했다. 굿모닝신한증권 김학균 연구원도 연휴기간 동안 필라델피아반도체지수가 약 4%정도의 낙폭을 보인 것이 오늘 삼성전자에 대한 외국인 매물로 이어지고 있다 면서 대만시장 또한 지난주말 외국인이 23거래일만에 순매도로 돌아서 아태 시장 전반에 대한 외국인의 매수열기가 식어가고 있다 고 설명했다. 김 연구원은 이어 오라클의 실적이 기대수준에 미치지 못한 것으로 발표되는등 미국시장은 고용없는 생산성 향상에 우려를 표시하고 있다 면서 오늘 중 지수는연휴기간 동안 미국시장의 하락 정도를 반영, 약 10포인트 가량의 낙폭을 보일 가능성이 크다 고 덧붙였다. \",\n",
              "   1]],\n",
              " [[' 달러-원 환율이 외국인의 역송금 수요로 1,090원대 후반으로 올랐다. 21일 서울 외환시장에서 오전 11시 10분 현재 달러-원 환율은 전 거래일 대비 1.80원 밀린 1,098.80원에 거래됐다. 개장 직후 1,097원대 부근에서는 달러 매도세가 다소 많이 나오기도 했다. 관련 물량이 소화되고서는 기존 원화 강세 흐름이 대체로 유지됐다. 이후 달러-원 환율은 글로벌 달러 약세가 되돌려지는 흐름과 맞물려 낙폭을 줄여나갔다. 외국인의 역송금 관련 달러 매수세가 유입됐다. 삼성전자 배당금은 아닌 것으로 추정됐다. ◇오후 전망 외환딜러들은 달러화가 1,095.00∼1,102.00원에서 등락할 것으로 예상했다. A은행의 한 외환딜러는 1,097원대에서 달러 매도세가 강했다 며 달러-엔 환율 등에 영향을 받지 않았나 한다 고 말했다. 이 딜러는 1,101.50원을 넘으면 1,102.50원까지 볼 수 있지만, 그 위는 어렵다 고 판단했다. B은행 딜러는 물량으로 올랐기 때문에 조금은 밀리지 않을까 한다 며 1,098원 부근에서 횡보할 것으로 예상한다 고 내다봤다. ◇장중 동향  달러화는 역외차액결제선물환 시장 환율을 반영해 전 거래일보다 3.60원 내린 1,097.00원에 출발했다. 개장 직후 달러-원 환율은 1,098원 선으로 조금 올랐고, 이 과정에서 딜미스가 생기기도 했다. 달러화는 1,097원대에서 다소 많은 매도 물량으로 1,096원대로 하락했다. 이어 외국인 역송금 물량이 시장에 나오면서 달러화를 위로 이끌었다.  오전 달러화 고점은 1,099.90원에 형성됐다. 같은 시각 외국인 투자자들은 유가증권시장에서 786억 원 규모의 주식을 순매수했고, 코스닥에서는 165억 원어치를 순매도했다. 달러-엔 환율은 뉴욕시장 대비 0.04엔 오른 112.67엔, 유로-달러 환율은 전일 대비 0.0005달러 오른 1.1732달러 거래됐다. 엔-원 재정환율은 100엔당 975.21원을 나타냈고, 위안-원 환율은 1위안당 165.31원에 거래됐다. ',\n",
              "   1],\n",
              "  [' G20 비즈니스 서밋은 4개 분과에 12개 워킹그룹 미팅과 공동선언문 채택 등 바쁘게 진행됐으나 대부분 성과는 96건에 이르는 개별 미팅 등을 통해 도출됐다.   대기업 회장들은 글로벌 기업 CEO와 만남을 통해 네트워크 구축한 것은 물론 앞으로 협력방안에 대한 구체적인 방안까지 논의했다.   정준양 포스코 회장은 쥬진 메철 회장과 지난 10일 청와대에서 극동시베리아지역자원개발에 관한 상호협력 양해각서를 체결했다. 메첼은 러시아의 유연탄 업체다. 올해 들어 호주의 석탄광산과 철광석 광산 지분을 사들이는 등 자원 확보에 총력전을 펼치는 포스코로서는 또 하나의 자원 확보의 교두보를 마련했다.   정 회장은 또 세계 최대 철광석 생산업체인 브라질 발레사의 로저 아그넬리 회장과 호주 리오틴토의 샘 월시 철광석 부문 총괄 부회장을 만났다. 두 기업 모두 포스코에 중요한 포스코의 원재료 제공업체다.   이석채 KT 회장은 10일 차이나모바일의 왕젠저우 회장을 만나 전략적 협력을 위한 협정서를 체결했다. 양사는 앞으로 와이파이 로밍, 차세대 네트워크·스마트폰 협력, 글로벌시장 공동 진출 등을 추진하기로 했다. 이 회장은 스페인 텔레포니카사의 세사르 알리에리타 회장과 대만 타이완모바일의 허베이창 회장 등과도 만나 차세대 네트워크에 대해 논의했다.   어윤대 KB금융지주 회장은 중국 공상은행의 장젠칭 회장과 개별 미팅에서 IB 분야 등에서 협력하기로 합의했다. 제휴 내용을 담은 서류가 오고 간 것은 아니지만, 실무진들이 공상은행을 방문해 구체적인 협력 방안을 논의할 예정이다.   정몽구 현대기아차그룹 회장은 9일 세계 최대의 자동차부품 업체인 독일 보쉬의프란츠 페렌바흐 회장을 만나 디젤 엔진과 전기자동차 배터리 등 신사업 분야에서 협력키로 했다. 페렌바흐 회장은 현대차의 수소연료전지차에 관심을 보인 것으로 알려졌다. 정 회장은 13일에는 터키의 레제프 타이이프 에르도안 총리와 면담해 현지 시장 진출 확대 방안을 논의했다.   이윤우 삼성전자 부회장도 중국 광저우 아시안게임에 참석하기 위해 출국한 이건희 회장을 대신해 퀄컴의 폴 제이콥스 회장, 시스코의 윔 엘프링크 부회장, 휴렛패커드 리처드 브래들리 부사장 등을 잇달아 접촉했다.   정만원 SK텔레콤 사장은 스마트폰 블랙베리를 개발한 캐나다 리서치인모션의 짐 발실리 CEO를 만나 협력 방안에 대한 의견을 나눴다.   이번 비즈니스 서밋에 참석하지는 않았으나 기회를 잘 활용한 기업도 있다.   김쌍수 한국전력 사장은 G20 비즈니스 서밋에 참석하기 위해 방한한 이탈리아 에넬사의 풀비오 콘티 에넬 사장과 스마트그리드와 이산화탄소 포장·저장 분야의 기술협력을 위한 의향서를 맺기도 했다. 또 현대제철은 메첼, 석유공사와 가스공사는 렙솔 YPF나 셸 등과 사업협력 방안을 논의한 것으로 전해졌다.   재계의 한 관계자는 협력 방안에 합의한 성과도 있었지만 400여명의 국내외 기업인이 만나 네트워크를 구축했다는 무형의 소득은 나중에 실질적인 협력으로 이어질것으로 기대된다 고 말했다.   ',\n",
              "   1],\n",
              "  [\" QLED 8K TV는 국제 규격에도 맞지 않으며 8K가 아닌 단순한 액정표시장치 TV에 불과하다 는 LG전자의 비판에 삼성전자가 적극적으로 반박하고 나섰다.  삼성전자는 17일 오전 LG전자가 디스플레이 기술설명회를 열어 자사의 QLED 8K TV에 대해 기술적으로 부족하다고 비판한 것에 대응해 같은 날 오후 8K 화질설명회를 열어 반격했다.  특히 LG전자가 화질선명도를 내세워 자사의 QLED 8K TV에 대해 '폄훼'한 것에 대해 송출 능력 데이터를 제시하면서 LG전자의 올레드가 기술적으로 부족하다며 맞불을 놨다.  이날 오후 서초구 우면동 서울 R&amp;D캠퍼스에서 열린 설명회에서 용석우 삼성전자 영상디스플레이사업부 개발팀 상무는 자사의 QLED 8K TV가 이미지와 동영상, 스트리밍 등 다양한 콘텐츠에서 LG전자의 올레드 8K TV에 비해 높은 성능을 보인다고 강조했다.  용석우 상무는 8K 이미지 파일을 USB에 옮겨 TV에 띄운 결과 삼성전자의 QLED TV에서는 작은 글씨도 선명하게 보이지만 LG전자 올레드 TV에서는 글씨가 뭉개지는 현상이 나타났다 고 말했다.  용 상무는 8K 카메라로 이미지를 촬영한 후 QLED TV와 올레드 TV에 이를 송출해 같은 현상이 발생하는 모습을 직접 시연해 보이기도 했다.  그는 표준코덱으로 인코딩된 8K 동영상 시연에서도 QLED 8K TV는 USB로 연결한 영상이든 스트리밍 영상이든 원활하게 재생이 되지만, 올레드 TV에서는 동영상 재생이 되지 않거나 화면이 깨지는 현상이 발생한다 고도 했다.  8K 화질을 LG전자가 주장처럼 화질선명도만으로 측정해서는 안 된다는 것을 강조한 것이다.  또 LG전자가 제시한 국제디스플레이계측위원회 화질선명도 기준은 1927년에 발표된 개념으로, 물리적으로 화소 수를 세기 어려운 디스플레이나 흑백 TV의 해상도 평가를 위해 사용된 것이라고 반박했다.  ICDM의 화질선명도 기준은 8K TV와 같은 초고해상도 컬러 디스플레이의 평가 기준에는 적합하지 않다는 것이 삼성전자의 주장이다.  앞서 LG전자는 이날 오전 설명회에서 삼성전자의 QLED 8K TV의 화질선명도가 50% 이하로, ICDM 규정 8K 화질선명도를 충족하지 못한다고 지적했다.  용 상무는 8K TV 화질은 화질선명도로 판단하는 것이 아니라 밝기와 컬러볼륨 등 다른 광학적인 요소와 화질 처리 기술 등 시스템적인 부분이 최적으로 조합돼야 한다 며 기준 정립을 위해 업체 간 협의가 활성화돼야 한다 고 말했다.  삼성전자는 8K 협회를 구성해 해상도와 최대 밝기, 전송 인터페이스, 압축 규격 등 8K TV와 관련된 구체적인 기준을 제시한 바 있다.  8K 협회는 TV와 패널 제조사뿐 아니라 콘텐츠 제작, 유통사를 포함해 16개사가 회원으로 참여하고 있다.  용 상무는 8K 시장이 성장하는 단계에서 소모적인 논쟁보다는 8K 협회에 더 많은 기업이 참여해 미래 시장을 만들어나가기를 희망한다 고 말했다.  \",\n",
              "   1],\n",
              "  [\" 서울외환시장 참가자들은 외국인 투자자들의 재투자 가능성에 따른 달러-원 하락 압력에 기대를 걸고 있다.기업들의 배당금 시즌이 이어지고 있지만 외국인 투자자들의 역송금 우려가 줄고 있어서다.국내 주식과 채권에 대한 외국인의 자금 유입이 우세한 상황이 이어지고 있다. 11일 서울환시 등에 따르면 달러화는 외국인 투자자들의 재투자 기대에 하락세를 이어갈 전망이다. 역외차액결제선물환 시장에서 달러-원 1개월물은 전 거래일 종가 대비 2.25원 하락 마감하면서 1,150원대 초반에 머물렀다. 국내주식시장에서 외국인들은 전 거래일인 지난 8일 코스피와 코스닥을 합쳐 1천770억원 순매수했고 채권은 5천623억원 매수했다. 외국인 투자자들은 배당금 시기를 맞아 오히려 주식 매수세를 높이면서 지난 6일부터 3거래일 연속으로 순매수 행진을 이어가고 있다. 같은 날 현대자동차와 SK하이닉스 및 현대모비스 등이 외국인 배당금을 지급했다. 삼성전자는 이날 1조4천억원 가량의 외국인 배당금을 지급한다, 포스코도 2천억원 가량 외국인 배당금을 내놓는다. 역송금 우려 등에 달러화가 장초반 상승 압력을 받기도 하지만 장중 롱스탑에 상승폭을 대거 반납하는 등 전강후약 장세가 반복되는 양상이다. 서울환시의 외환딜러들은 거시건전성 3종 세트 등 국내 규제 개선과 함께 국내 기업의 어닝 서프라이즈 등으로 원화 자산의 매력이 커졌다고 보고 역송금 가능성보다는 주식 및 채권 재투자 등 자금 유입 가능성을 더 크게 봤다. 기획재정부는 지난해 '2016년 경제정책방향'을 발표하고 미국의 금리 인상 등으로 시장변동성이 확대될 것에 대비해 외환건전성 관리제도를 원점에서 재검토하겠다고 밝힌 바 있다. 거시건전성 3종 세트가 올해 상반기에 개편될 예정이다.  A 시중은행 외환딜러는 외국인들이 국내 주식 순매수 전환하면서 역외시장 참가자들도 달러를 팔고 있다 며 외국인들이 배당금을 받더라도 역송금 수요로 인한 자금 이탈 가능성이 크다고 보진 않는다 고 말했다.  B 시중은행 외환딜러는 외인 주식 매수세가 계속되는 것을 보니 받은 배당금으로 주식 재투자했을 가능성이 크다 며 역송금 수요에 1,155원까진 반등할 것이라는 기대가 컸지만 장중 롱스탑에 달러화가 미끄러지는 장세가 이어지고 있다 고 말했다. 딜러들은 신흥국 시장에서 한국 시장의 안정성과 향후 외환 관련 규제 개선 가능성 등을 강조했다. 재닛 옐런 연방준비제도 의장의 비둘기파적인 발언 이후 연준의 금리 스탠스를 확인한 상황에서 원화 자산의 보유 유인도 커지고 있다는 지적이 이어졌다. B 은행 딜러는 우리나라 거시건전성 3종 세트도 완화되면 앞으로 외국인 투자가 들어오기가 더 편해질 것 이라며 여기에 또 금리가 아직까진 선진국 시장보다 높아 채권을 사는 등 자금이 재유입될 가능성이 크다 고 말했다. 평가 이익도 만만치 않은 상황이다. 달러로 환전해 원화 주식을 살 경우 원화 가치가 오를 경우 환차익이 발생한다. 지난 2월 1,245.30원까지 올랐던 달러화가 몇달 사이에 100원 가까이 하락하면서 우리나라 주식 가치의 평가 이익이 높아졌다. C 시중은행 외환딜러는 외국인 투자자들이 배당금을 받았지만 신흥국 시장만 놓고 봤을 때 굳이 우리나라에 자금을 빼서 싱가포르나 홍콩, 중국 쪽으로 포트폴리오를 조정할 만큼 우리나라 시장이 불안하지 않다 며 현재 달러화 레벨이 하락하면서 환율상으로 평가익도 꽤 받았을 것이다 고 말했다. 그는 외국인 투자자들이 배당금을 환전해 역송금할 가능성보다는 우리나라 원화 계정에 뒀다가 주식이 하락하면 재투자할 가능성이 크다 고 덧붙였다. \",\n",
              "   1],\n",
              "  [\" 국내에서 내로라하는 대기업 총수들이 문재인 대통령이 청와대에 초청한 형식으로 열릴 '2019 기업인과의 대화'에 총출동한다.  대한상공회의소는 15일 열리는 '2019년 기업인과의 대화'에 참가하는 기업인 명단을 14일 발표했다.  참가하는 기업인은 박용만 대한상의 회장을 비롯한 전국상공회의소 회장단 67명을 비롯해 대기업 22명, 중견기업 39명 등 총 128명으로 정해졌다.  특히 대기업에서는 이재용 삼성전자 부회장, 정의선 현대자동차 수석부회장, 최태원 SK 회장, 구광모 LG 회장 등 국내 4대 그룹의 총수들이 모두 이름을 올렸다.  여기에 신동빈 롯데 회장과 최정우 포스코 회장, 허창수 GS 회장, 김승연 한화 회장, 김병원 농협 회장, 권오갑 현대중공업 부회장 등 10대 그룹의 회장 및 부회장도 일제히 포함됐다. 정용진 신세계 부회장, 황창규 KT 회장, 박정원 두산 회장, 손경식 CJ 회장, 구자열 LS 회장, 정지선 현대백화점 회장, 박삼구 금호아시아나 회장 등 주요 대기업 회장들도 대부분 참가한다.  중견기업에서는 정몽원 한라 회장과 박인구 동원그룹 부회장, 손정원 한온시스템 대표, 우오현 SM그룹 회장, 윤석금 웅진 회장, 방준혁 넷마블 의장, 함영준 오뚜기 회장, 김택진 엔씨소프트 회장, 김석준 쌍용건설 회장, 서정진 셀트리온 회장 등 업종별로 우리나라를 대표하는 중견기업인 39명이 포함됐다.  아울러 현정은 현대그룹 회장과 서민석 동일방직 회장, 김희용 동양물산기업 회장, 배동현 아모레퍼시픽 대표 등은 서울상공회의소 회장단으로 명단에 포함됐다.  이번 '기업인과의 대화'는 타운홀 미팅 방식으로 열린다.  박용만 대한상의 회장의 진행으로, 기업인들은 청와대·정부·여당과 각종 현안을 자유토론하고 질의·응답할 예정이다.  대한상의는 사전 시나리오 없는 자유로운 형식으로 대기업과 중견기업, 지역상공인들이 산업현장의 생생한 목소리를 허심탄회하게 전달할 예정 이라고 밝혔다.  대한상의는 사상 유례없는 방식으로 열리는 이번 기업인 대화를 통해 경제활력 회복의 물꼬를 트는 다양한 해결책이 마련되길 기대한다 고 덧붙였다.  \",\n",
              "   1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7K-e703qmYe"
      },
      "source": [
        "## **데이터 토큰화**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCyfeDYxX-Ex"
      },
      "source": [
        "ctx= 'cuda'\n",
        "cachedir='~/kogpt2/'\n",
        "pytorch_kobert = {\n",
        "    'url': 'https://kobert.blob.core.windows.net/models/kobert/pytorch/pytorch_kobert_2439f391a6.params',\n",
        "    'fname': 'pytorch_kobert_2439f391a6.params',\n",
        "    'chksum': '2439f391a6'\n",
        "}\n",
        "\n",
        "bert_config = {\n",
        "    'attention_probs_dropout_prob': 0.1,\n",
        "    'hidden_act': 'gelu',\n",
        "    'hidden_dropout_prob': 0.1,\n",
        "    'hidden_size': 768,\n",
        "    'initializer_range': 0.02,\n",
        "    'intermediate_size': 3072,\n",
        "    'max_position_embeddings': 512,\n",
        "    'num_attention_heads': 12,\n",
        "    'num_hidden_layers': 12,\n",
        "    'type_vocab_size': 2,\n",
        "    'vocab_size': 8002\n",
        "}"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1Ve5qwwX_M1",
        "outputId": "194912fd-9429-43fd-a774-720a6bce11e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        }
      },
      "source": [
        "# download model\n",
        "model_info = pytorch_kobert\n",
        "model_path = download(model_info['url'],\n",
        "                      model_info['fname'],\n",
        "                      model_info['chksum'],\n",
        "                      cachedir=cachedir)\n",
        "# download vocab\n",
        "vocab_info = tokenizer\n",
        "vocab_path = download(vocab_info['url'],\n",
        "                      vocab_info['fname'],\n",
        "                      vocab_info['chksum'],\n",
        "                      cachedir=cachedir)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-75a04629c02a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# download model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpytorch_kobert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m model_path = _download(model_info['url'],\n\u001b[0m\u001b[1;32m      4\u001b[0m                       \u001b[0mmodel_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fname'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                       \u001b[0mmodel_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'chksum'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name '_download' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF_FP8hKqkO8",
        "tags": [],
        "outputId": "8533161b-335a-49bb-ded4-889a321b4510",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "bertmodel, vocab = get_pytorch_kobert_model()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using cached model\n",
            "using cached model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-05831617e7fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'cuda'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbertmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pytorch_kobert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcachedir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'~/kobert/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kobert/pytorch_kobert.py\u001b[0m in \u001b[0;36mget_pytorch_kobert_model\u001b[0;34m(ctx, cachedir)\u001b[0m\n\u001b[1;32m     62\u001b[0m                            \u001b[0mvocab_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'chksum'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                            cachedir=cachedir)\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_kobert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kobert/pytorch_kobert.py\u001b[0m in \u001b[0;36mget_kobert_model\u001b[0;34m(model_file, vocab_file, ctx)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_kobert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mbertmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBertConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mbertmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mbertmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1045\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1046\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BertModel:\n\tMissing key(s) in state_dict: \"embeddings.position_ids\". "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qo1zrFkvqul-",
        "tags": [],
        "outputId": "e9dc1b12-a7b5-4f32-9ee9-8098714b9a3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using cached model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-3c868c3b0952>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBERTSPTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eol3wYgBqu0V"
      },
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
        "                 pad, pair):\n",
        "        transform = nlp.data.BERTSentenceTransform(\n",
        "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
        "\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4gfqqCfqwSC"
      },
      "source": [
        "# 파라미터 세팅\n",
        "max_len = 64\n",
        "batch_size = 64\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 5\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate =  5e-5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR9vAi4Iq1pc"
      },
      "source": [
        "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
        "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_u6kfBjq2Rw"
      },
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
        "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYGu4Pwoq42j"
      },
      "source": [
        "## **모델링**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbH9U97Cq4cO"
      },
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes=2, # 해당 부분 파라미터 조정으로 다중 분류 가능\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "                 \n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "    \n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "        \n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hzKl-wwq-TF"
      },
      "source": [
        "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsK3WOz1q_wF"
      },
      "source": [
        "# Prepare optimizer and schedule (linear warmup and decay)\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWvxDswmrA7e"
      },
      "source": [
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjcCf5RYrCUL"
      },
      "source": [
        "t_total = len(train_dataloader) * num_epochs\n",
        "warmup_step = int(t_total * warmup_ratio)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7amz9K9rDix"
      },
      "source": [
        "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=warmup_step, t_total=t_total) # Warm-Up Step 추가"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAUHxhxUrE5D"
      },
      "source": [
        "def calc_accuracy(X,Y): # 정확도 계산 함수\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EKdgAmKrHPD"
      },
      "source": [
        "## **모델 학습**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dR1T-8W3re5J",
        "tags": [],
        "outputId": "a9ba8346-6f55-414f-f178-fa028b373767",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524,
          "referenced_widgets": [
            "20830e614d3145fbad9ce556c4075c6f",
            "a4c66db09af9483ea664bb38798d90ca",
            "548a3cc4f2ea45189d0a18ff28d8af62",
            "baaef858734d4799874234f1d931c72d",
            "e666a318077e4657ac9498767cb16e8d",
            "99da2490ef85491eb505013778e5b574",
            "727751458af245bfb5c1bb021a7120d7",
            "e063fef3ad6e4ce2ba3966bf966de205",
            "22707fec1b8c4d6e9b3c674210417bd7",
            "2924363e19554080adea4b08ae374de9",
            "76a18fc102884893a3e5cdff6c7a64b6",
            "10d03528effb4701b11573cf9359d2e3",
            "a5d31686ef11439f90ebfc1ebd0ea5a5"
          ]
        }
      },
      "source": [
        "for e in range(num_epochs):\n",
        "    train_acc = 0.0\n",
        "    test_acc = 0.0\n",
        "    # 모델 학습 : model.train()을 지정해줘야 Fine Tunning이 이뤄짐\n",
        "    model.train()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
        "        optimizer.zero_grad()\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        loss = loss_fn(out, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update learning rate schedule\n",
        "        train_acc += calc_accuracy(out, label)\n",
        "        if batch_id % log_interval == 0:\n",
        "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
        "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
        "    # 모델 평가 : model.eval()을 지정해야 Fine Tunning을 멈추고 평가를 시작함\n",
        "    model.eval()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        test_acc += calc_accuracy(out, label)\n",
        "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n",
        "    # 에폭마다 체크포인트 저장\n",
        "    PATH = \"/home/lab12/workspace/bigcontest/data_analysis/model_checkpoint/{}_epochs/\".format(e+1) # 모델 저장 경로 설정\n",
        "    torch.save(model, PATH + 'model.pt')  # 전체 모델 저장\n",
        "    torch.save(model.state_dict(), PATH + 'model_state_dict.pt')  # 모델 객체의 state_dict 저장\n",
        "    torch.save({\n",
        "                'model': model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict()\n",
        "                }, PATH + 'all.tar')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=10213.0), HTML(value='')))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22707fec1b8c4d6e9b3c674210417bd7"
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 2 batch id 1 loss 0.554951548576355 train acc 0.71875\n",
            "epoch 2 batch id 201 loss 0.5218873620033264 train acc 0.7101212686567164\n",
            "epoch 2 batch id 401 loss 0.45010921359062195 train acc 0.7061253117206983\n",
            "epoch 2 batch id 601 loss 0.6309182643890381 train acc 0.7065307820299501\n",
            "epoch 2 batch id 801 loss 0.5050973296165466 train acc 0.7078261548064919\n",
            "epoch 2 batch id 1001 loss 0.5672126412391663 train acc 0.7094155844155844\n",
            "epoch 2 batch id 1201 loss 0.5219614505767822 train acc 0.7114253746877602\n",
            "epoch 2 batch id 1401 loss 0.5552719831466675 train acc 0.7125044610992148\n",
            "epoch 2 batch id 1601 loss 0.4990692138671875 train acc 0.7138799968769519\n",
            "epoch 2 batch id 1801 loss 0.5215151309967041 train acc 0.7158002498611882\n",
            "epoch 2 batch id 2001 loss 0.42466288805007935 train acc 0.7175630934532734\n",
            "epoch 2 batch id 2201 loss 0.567004382610321 train acc 0.7188067923671059\n",
            "epoch 2 batch id 2401 loss 0.4554973542690277 train acc 0.7199344023323615\n",
            "epoch 2 batch id 2601 loss 0.5368044972419739 train acc 0.7212370242214533\n",
            "epoch 2 batch id 2801 loss 0.5410217046737671 train acc 0.7221918511245984\n",
            "epoch 2 batch id 3001 loss 0.431264191865921 train acc 0.7236598217260913\n",
            "epoch 2 batch id 3201 loss 0.36482948064804077 train acc 0.7249541159012809\n",
            "epoch 2 batch id 3401 loss 0.6356601119041443 train acc 0.7259583578359307\n",
            "epoch 2 batch id 3601 loss 0.5150079727172852 train acc 0.7269812204943071\n",
            "epoch 2 batch id 3801 loss 0.5140596032142639 train acc 0.7280197645356485\n",
            "epoch 2 batch id 4001 loss 0.6080747842788696 train acc 0.7290364908772807\n",
            "epoch 2 batch id 4201 loss 0.4021480083465576 train acc 0.7296662996905499\n",
            "epoch 2 batch id 4401 loss 0.5034851431846619 train acc 0.7300649000227221\n",
            "epoch 2 batch id 4601 loss 0.4489150941371918 train acc 0.7306665670506411\n",
            "epoch 2 batch id 4801 loss 0.62971031665802 train acc 0.7311107061028952\n",
            "epoch 2 batch id 5001 loss 0.5255213379859924 train acc 0.7317567736452709\n",
            "epoch 2 batch id 5201 loss 0.4271070957183838 train acc 0.7324522928283023\n",
            "epoch 2 batch id 5401 loss 0.5666614770889282 train acc 0.7326623541936679\n",
            "epoch 2 batch id 5601 loss 0.5322099328041077 train acc 0.7331810167827174\n",
            "epoch 2 batch id 5801 loss 0.38980185985565186 train acc 0.7336235131873815\n",
            "epoch 2 batch id 6001 loss 0.4831678867340088 train acc 0.7336771996333944\n",
            "epoch 2 batch id 6201 loss 0.5178375840187073 train acc 0.7340575108853411\n",
            "epoch 2 batch id 6401 loss 0.622481107711792 train acc 0.7342798000312452\n",
            "epoch 2 batch id 6601 loss 0.5643627047538757 train acc 0.7345572640509014\n",
            "epoch 2 batch id 6801 loss 0.5090881586074829 train acc 0.7350872114394942\n",
            "epoch 2 batch id 7001 loss 0.575127363204956 train acc 0.7355645622053992\n",
            "epoch 2 batch id 7201 loss 0.5899335145950317 train acc 0.735896056103319\n",
            "epoch 2 batch id 7401 loss 0.47351664304733276 train acc 0.7360365153357654\n",
            "epoch 2 batch id 7601 loss 0.4579690098762512 train acc 0.7362435863702145\n",
            "epoch 2 batch id 7801 loss 0.543423593044281 train acc 0.736578243173952\n",
            "epoch 2 batch id 8001 loss 0.5301893949508667 train acc 0.7366735408073991\n",
            "epoch 2 batch id 8201 loss 0.4892342686653137 train acc 0.73691280026826\n",
            "epoch 2 batch id 8401 loss 0.44787561893463135 train acc 0.7367798476371861\n",
            "epoch 2 batch id 8601 loss 0.4464476704597473 train acc 0.7368456429484943\n",
            "epoch 2 batch id 8801 loss 0.47195354104042053 train acc 0.7372067094648336\n",
            "epoch 2 batch id 9001 loss 0.645601212978363 train acc 0.7374857654705033\n",
            "epoch 2 batch id 9201 loss 0.5858950018882751 train acc 0.7377543881099881\n",
            "epoch 2 batch id 9401 loss 0.5362008213996887 train acc 0.7379450989256462\n",
            "epoch 2 batch id 9601 loss 0.46711722016334534 train acc 0.738147393500677\n",
            "epoch 2 batch id 9801 loss 0.3929792046546936 train acc 0.7384737526782982\n",
            "epoch 2 batch id 10001 loss 0.5730574131011963 train acc 0.7386245750424958\n",
            "epoch 2 batch id 10201 loss 0.457459032535553 train acc 0.7387036197431625\n",
            "\n",
            "epoch 2 train acc 0.7386985337315186\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2554.0), HTML(value='')))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2924363e19554080adea4b08ae374de9"
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 2 test acc 0.6849121476115897\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=10213.0), HTML(value='')))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76a18fc102884893a3e5cdff6c7a64b6"
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 3 batch id 1 loss 0.5583212375640869 train acc 0.71875\n",
            "epoch 3 batch id 201 loss 0.5698355436325073 train acc 0.7060789800995025\n",
            "epoch 3 batch id 401 loss 0.41047367453575134 train acc 0.7082294264339152\n",
            "epoch 3 batch id 601 loss 0.6113224625587463 train acc 0.7093646006655574\n",
            "epoch 3 batch id 801 loss 0.5261039137840271 train acc 0.7105376092384519\n",
            "epoch 3 batch id 1001 loss 0.5396313071250916 train acc 0.7109921328671329\n",
            "epoch 3 batch id 1201 loss 0.4963166415691376 train acc 0.7133248334721066\n",
            "epoch 3 batch id 1401 loss 0.4880925416946411 train acc 0.7150361349036403\n",
            "epoch 3 batch id 1601 loss 0.500450849533081 train acc 0.7165053091817614\n",
            "epoch 3 batch id 1801 loss 0.5097231268882751 train acc 0.7186632426429761\n",
            "epoch 3 batch id 2001 loss 0.4262673854827881 train acc 0.7210691529235382\n",
            "epoch 3 batch id 2201 loss 0.6262375712394714 train acc 0.7230946160835984\n",
            "epoch 3 batch id 2401 loss 0.4919337034225464 train acc 0.7249648583923365\n",
            "epoch 3 batch id 2601 loss 0.5172901749610901 train acc 0.7264573721645521\n",
            "epoch 3 batch id 2801 loss 0.5136399865150452 train acc 0.7283559443056051\n",
            "epoch 3 batch id 3001 loss 0.43187880516052246 train acc 0.7306626957680773\n",
            "epoch 3 batch id 3201 loss 0.3724531829357147 train acc 0.7331058653545767\n",
            "epoch 3 batch id 3401 loss 0.7392411828041077 train acc 0.7347333504851514\n",
            "epoch 3 batch id 3601 loss 0.4841882586479187 train acc 0.7368048805887254\n",
            "epoch 3 batch id 3801 loss 0.5008565783500671 train acc 0.739184589581689\n",
            "epoch 3 batch id 4001 loss 0.6166249513626099 train acc 0.7412053236690828\n",
            "epoch 3 batch id 4201 loss 0.4445038139820099 train acc 0.7430299333492025\n",
            "epoch 3 batch id 4401 loss 0.4535479247570038 train acc 0.7443443251533742\n",
            "epoch 3 batch id 4601 loss 0.4364050328731537 train acc 0.7458195229297979\n",
            "epoch 3 batch id 4801 loss 0.4864228665828705 train acc 0.7471425223911685\n",
            "epoch 3 batch id 5001 loss 0.5896709561347961 train acc 0.7489189662067587\n",
            "epoch 3 batch id 5201 loss 0.4010394811630249 train acc 0.750648913670448\n",
            "epoch 3 batch id 5401 loss 0.5137646794319153 train acc 0.7521292353267913\n",
            "epoch 3 batch id 5601 loss 0.46946191787719727 train acc 0.75366285038386\n",
            "epoch 3 batch id 5801 loss 0.3134530186653137 train acc 0.7551796026547147\n",
            "epoch 3 batch id 6001 loss 0.3936918079853058 train acc 0.7561396017330445\n",
            "epoch 3 batch id 6201 loss 0.4994165599346161 train acc 0.7574357966457023\n",
            "epoch 3 batch id 6401 loss 0.5119683146476746 train acc 0.7585655561630995\n",
            "epoch 3 batch id 6601 loss 0.595924437046051 train acc 0.7596150204514468\n",
            "epoch 3 batch id 6801 loss 0.3955787420272827 train acc 0.7610737391560065\n",
            "epoch 3 batch id 7001 loss 0.40268629789352417 train acc 0.7622884230824168\n",
            "epoch 3 batch id 7201 loss 0.5056182146072388 train acc 0.7633553499513956\n",
            "epoch 3 batch id 7401 loss 0.3649681508541107 train acc 0.7642527192271314\n",
            "epoch 3 batch id 7601 loss 0.3071565628051758 train acc 0.7651542560189448\n",
            "epoch 3 batch id 7801 loss 0.4875037372112274 train acc 0.7660916869632098\n",
            "epoch 3 batch id 8001 loss 0.3632013499736786 train acc 0.7666717441569804\n",
            "epoch 3 batch id 8201 loss 0.36747318506240845 train acc 0.7674521399829289\n",
            "epoch 3 batch id 8401 loss 0.3756392002105713 train acc 0.7680633257945483\n",
            "epoch 3 batch id 8601 loss 0.3288038969039917 train acc 0.76862610452273\n",
            "epoch 3 batch id 8801 loss 0.33923667669296265 train acc 0.7695166032269061\n",
            "epoch 3 batch id 9001 loss 0.6318771839141846 train acc 0.7701800494389512\n",
            "epoch 3 batch id 9201 loss 0.48237770795822144 train acc 0.7709318280621672\n",
            "epoch 3 batch id 9401 loss 0.3810237646102905 train acc 0.7715435857887458\n",
            "epoch 3 batch id 9601 loss 0.3438604772090912 train acc 0.7722486589938548\n",
            "epoch 3 batch id 9801 loss 0.3226066827774048 train acc 0.7729919140903989\n",
            "epoch 3 batch id 10001 loss 0.41566985845565796 train acc 0.7736804444555544\n",
            "epoch 3 batch id 10201 loss 0.40036630630493164 train acc 0.7741918684442701\n",
            "\n",
            "epoch 3 train acc 0.7742124008616469\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2554.0), HTML(value='')))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10d03528effb4701b11573cf9359d2e3"
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 3 test acc 0.6966400743931088\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=10213.0), HTML(value='')))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a5d31686ef11439f90ebfc1ebd0ea5a5"
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 4 batch id 1 loss 0.37403547763824463 train acc 0.875\n",
            "epoch 4 batch id 201 loss 0.3503040671348572 train acc 0.7706001243781094\n",
            "epoch 4 batch id 401 loss 0.3806527554988861 train acc 0.7722880299251871\n",
            "epoch 4 batch id 601 loss 0.5747219324111938 train acc 0.7731905158069884\n",
            "epoch 4 batch id 801 loss 0.43526554107666016 train acc 0.7751638576779026\n",
            "epoch 4 batch id 1001 loss 0.4218243956565857 train acc 0.7764110889110889\n",
            "epoch 4 batch id 1201 loss 0.38353922963142395 train acc 0.7787650915903414\n",
            "epoch 4 batch id 1401 loss 0.4645388424396515 train acc 0.7800901142041399\n",
            "epoch 4 batch id 1601 loss 0.4076917767524719 train acc 0.7815427857589007\n",
            "epoch 4 batch id 1801 loss 0.3772083818912506 train acc 0.7835490699611327\n",
            "epoch 4 batch id 2001 loss 0.3051791787147522 train acc 0.7855369190404797\n",
            "epoch 4 batch id 2201 loss 0.4458267092704773 train acc 0.7877527260336211\n",
            "epoch 4 batch id 2401 loss 0.2519046366214752 train acc 0.7895017700957934\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-a0971add402d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/home/ubuntu/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}